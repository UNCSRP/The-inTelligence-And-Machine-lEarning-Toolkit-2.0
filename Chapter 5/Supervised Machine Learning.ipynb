{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad7d069",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning \n",
    "\n",
    "This training module was developed by Alexis Payton, MS, Oyemwenosa N. Avenbuan, and Dr. Julia E. Rager\n",
    "\n",
    "Spring 2023\n",
    "\n",
    "## Introduction to Training Module\n",
    "Machine learning is a field that has been around for decades, but has exploded in popularity and utility in recent years due to proliferation of big data. Through the building of models, machine learning has the ability to sift through and learn from large volumes of data and use that knowledge to solve problems. The challenges of big and high dimensional data as they pertain to environmental health and how machine learning can mitigate some of those challenges are discussed further in [Payton et. al](https://www.frontiersin.org/articles/10.3389/ftox.2023.1171175/full).\n",
    "\n",
    "In this training module, we will explore:\n",
    "+ Types of machine learning\n",
    "+ Building/ training a supervised machine learning algorithm\n",
    "+ Assessment of a supervised machine learning model's performance\n",
    "\n",
    "Supervised machine learning will be explored using a geology dataset. This example dataset was generated by measuring inorganic Arsenic (iAs) concentrations and other variables in 713 private wells across North Carolina. Let's go ahead and view these data. \n",
    "\n",
    "### Installing required R packages\n",
    "If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (!requireNamespace(\"readxl\"))\n",
    "  install.packages(\"readxl\");\n",
    "if (!requireNamespace(\"lubridate\"))\n",
    "  install.packages(\"lubridate\");\n",
    "if (!requireNamespace(\"tidyverse\"))\n",
    "  install.packages(\"tidyverse\");\n",
    "if (!requireNamespace(\"gtsummary\"))\n",
    "  install.packages(\"gtsummary\");\n",
    "if (!requireNamespace(\"caret\"))\n",
    "  install.packages(\"caret\");\n",
    "if (!requireNamespace(\"e1071\"))\n",
    "  install.packages(\"e1071\");\n",
    "if (!requireNamespace(\"Hmsic\"))\n",
    "  install.packages(\"Hmsic\");\n",
    "if (!requireNamespace(\"randomForest\"))\n",
    "  install.packages(\"randomForest\");\n",
    "if (!requireNamespace(\"pROC\"))\n",
    "  install.packages(\"pROC\");\n",
    "if (!requireNamespace(\"themis\"))\n",
    "  install.packages(\"themis\");\n",
    "if (!requireNamespace(\"rlang\"))\n",
    "  install.packages(\"rlang\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ff895",
   "metadata": {},
   "source": [
    "### Loading required R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d30a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(readxl);\n",
    "library(lubridate);\n",
    "library(tidyverse);\n",
    "library(gtsummary);\n",
    "library(caret);\n",
    "library(e1071);\n",
    "library(Hmisc);\n",
    "library(randomForest);\n",
    "library(pROC);\n",
    "library(themis);\n",
    "library(rlang);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5445cf",
   "metadata": {},
   "source": [
    "### Set your working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcf387",
   "metadata": {},
   "outputs": [],
   "source": [
    "setwd(\"/filepath to where your input files are\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0440d6",
   "metadata": {},
   "source": [
    "### Importing example dataset\n",
    "**Will need to change input to module number and add module number to the file itself**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e0d612c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 8</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Tax_ID</th><th scope=col>Water_Sample_Date</th><th scope=col>Casing_Depth</th><th scope=col>Well_Depth</th><th scope=col>Static_Water_Depth</th><th scope=col>Flow_Rate</th><th scope=col>pH</th><th scope=col>Detect_Concentration</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1006004</td><td>9/24/12 </td><td>52</td><td>165</td><td>41</td><td>60.0</td><td>7.7</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1024009</td><td>12/17/15</td><td>40</td><td>445</td><td>42</td><td> 2.0</td><td>7.3</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>1054019</td><td>2/2/15  </td><td>45</td><td>160</td><td>40</td><td>40.0</td><td>7.4</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1057017</td><td>10/22/12</td><td>42</td><td>440</td><td>57</td><td> 1.5</td><td>8.0</td><td>D </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>1060006</td><td>1/3/11  </td><td>48</td><td>120</td><td>42</td><td>25.0</td><td>7.1</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>1066006</td><td>12/15/15</td><td>60</td><td>280</td><td>32</td><td>10.0</td><td>8.2</td><td>D </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 8\n",
       "\\begin{tabular}{r|llllllll}\n",
       "  & Tax\\_ID & Water\\_Sample\\_Date & Casing\\_Depth & Well\\_Depth & Static\\_Water\\_Depth & Flow\\_Rate & pH & Detect\\_Concentration\\\\\n",
       "  & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & 1006004 & 9/24/12  & 52 & 165 & 41 & 60.0 & 7.7 & ND\\\\\n",
       "\t2 & 1024009 & 12/17/15 & 40 & 445 & 42 &  2.0 & 7.3 & ND\\\\\n",
       "\t3 & 1054019 & 2/2/15   & 45 & 160 & 40 & 40.0 & 7.4 & ND\\\\\n",
       "\t4 & 1057017 & 10/22/12 & 42 & 440 & 57 &  1.5 & 8.0 & D \\\\\n",
       "\t5 & 1060006 & 1/3/11   & 48 & 120 & 42 & 25.0 & 7.1 & ND\\\\\n",
       "\t6 & 1066006 & 12/15/15 & 60 & 280 & 32 & 10.0 & 8.2 & D \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 8\n",
       "\n",
       "| <!--/--> | Tax_ID &lt;chr&gt; | Water_Sample_Date &lt;chr&gt; | Casing_Depth &lt;dbl&gt; | Well_Depth &lt;dbl&gt; | Static_Water_Depth &lt;dbl&gt; | Flow_Rate &lt;dbl&gt; | pH &lt;dbl&gt; | Detect_Concentration &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 1006004 | 9/24/12  | 52 | 165 | 41 | 60.0 | 7.7 | ND |\n",
       "| 2 | 1024009 | 12/17/15 | 40 | 445 | 42 |  2.0 | 7.3 | ND |\n",
       "| 3 | 1054019 | 2/2/15   | 45 | 160 | 40 | 40.0 | 7.4 | ND |\n",
       "| 4 | 1057017 | 10/22/12 | 42 | 440 | 57 |  1.5 | 8.0 | D  |\n",
       "| 5 | 1060006 | 1/3/11   | 48 | 120 | 42 | 25.0 | 7.1 | ND |\n",
       "| 6 | 1066006 | 12/15/15 | 60 | 280 | 32 | 10.0 | 8.2 | D  |\n",
       "\n"
      ],
      "text/plain": [
       "  Tax_ID  Water_Sample_Date Casing_Depth Well_Depth Static_Water_Depth\n",
       "1 1006004 9/24/12           52           165        41                \n",
       "2 1024009 12/17/15          40           445        42                \n",
       "3 1054019 2/2/15            45           160        40                \n",
       "4 1057017 10/22/12          42           440        57                \n",
       "5 1060006 1/3/11            48           120        42                \n",
       "6 1066006 12/15/15          60           280        32                \n",
       "  Flow_Rate pH  Detect_Concentration\n",
       "1 60.0      7.7 ND                  \n",
       "2  2.0      7.3 ND                  \n",
       "3 40.0      7.4 ND                  \n",
       "4  1.5      8.0 D                   \n",
       "5 25.0      7.1 ND                  \n",
       "6 10.0      8.2 D                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "arsenic_data <- data.frame(read_excel(\"Module5/Arsenic_Data.xlsx\"))\n",
    "\n",
    "# View the top of the dataset\n",
    "head(arsenic_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc320e75",
   "metadata": {},
   "source": [
    "The columns in this dataset are described below:\n",
    "+ `Tax_ID`: Tax ID for the property\n",
    "+ `Water_Sample_ID`: Date that the well was sampled \n",
    "+ `Casing_Depth`: Depth of the casing of the well (ft)\n",
    "+ `Well_Depth`: Depth of the well (ft)\n",
    "+ `Static_Water_Depth`: Static water depth in the well (ft)\n",
    "+ `Flow_Rate`: Well flow rate (gallons per minute)\n",
    "+ `pH`: pH of water sample\n",
    "+ `Detect_Concentration`: Binary identifier (either non-detect (ND) or detect (D)) if As concentration detected in water sample \n",
    "\n",
    "\n",
    "## Training Module's Environmental Health Questions\n",
    "\n",
    "This training module was specifically developed to answer the following environmental health questions:\n",
    "\n",
    "1. Can we predict if iAs will be detected based on well water data? \n",
    "\n",
    "Before we dive into the code, let's get an understanding for what machine learning is, how to train a model, and assess its model performance.\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "Machine learning is a field of study in computer science that involves creating algorithms, which are a set of instructions that perform a specific task on a given dataset. These algorithms enable researchers to create models that can automatically analyze to new and unforeseen situations capable of improving automatically through experience and data.\n",
    "\n",
    "In other words, instead of being explicitly programmed to perform a task, a machine learning algorithm is designed to learn from examples and data, allowing it to adapt and improve over time. This approach is particularly useful for tasks that are too complex or difficult to be solved using traditional programming methods. **NOSA - I'M NOT SURE WHAT YOU MEANT HERE BY TRADITIONAL PROGRAMMING METHODS**\n",
    "\n",
    "Through machine learning, scientists can:\n",
    "\n",
    "+ Create a model that adapts to new circumstances \n",
    "+ Detect patterns in large and complex datasets\n",
    "+ Evaluate the effectiveness of these patterns \n",
    "+ Make informed decisions about how to improve their models\n",
    "\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "Within the field of machine learning, there are two types that are most commonly used in environmental toxicology research: supervised machine learning and unsupervised machine learning.\n",
    "\n",
    "**Supervised machine learning** involves training a model using a labeled dataset, where each dependent or predictor variable is associated with an independent variable with a known outcome. This allows the model to learn how to predict the labeled outcome on data it hasn't \"seen\" before based on the patterns and relationships it previously identified in the data. For example, supervised machine learning has been used for cancer prediction and prognosis based on variables like tumor size, stage, and age ([Lynch et. al](https://www.sciencedirect.com/science/article/abs/pii/S1386505617302368?via%3Dihub), [Asadi et. al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7416093/)). \n",
    "\n",
    "Supervised machine learning includes: \n",
    "+ Classification: Using algorithms to classify a categorical outcome (ie. plant species, disease status, etc.)\n",
    "+ Regression: Using algorithms to predict a continuous outcome (ie. temperature, chemical concentration, etc.)\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/3c22a8fc-ada5-4199-9967-77f504d99d2a\" width=\"684\" />\n",
    "\n",
    "([Soni, 2018](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d))\n",
    "\n",
    "\n",
    "**Unsupervised machine learning**, on the other hand, involves using models to find patterns or associations between variables in dataset that lack a known or labeled outcome. For example, unsupervised machine learning has been used to find associations of co-expressed genes within various biological pathways ([Botía et. al](https://bmcsystbiol.biomedcentral.com/articles/10.1186/s12918-017-0420-6), [Pagnuco et. al](https://www.sciencedirect.com/science/article/pii/S0888754317300575?via%3Dihub)).\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/4f78adef-97b6-425f-9a51-43315b0fb7b2\" width=\"684\" />\n",
    "\n",
    "([Langs et. al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6244522/))\n",
    "\n",
    "It's worth noting that there are also other types of machine learning, including semi-supervised learning, reinforcement learning and deep learning, which won't be discussed in these training modules. Overall, the distinction between supervised and unsupervised learning is an important concept in machine learning, as it can inform the choice of algorithms and techniques used to analyze and make predictions from data.\n",
    "\n",
    "To learn more check out the following resources: **I WILL LIKELY CHANGE THESE RESOURCES...I THINK WE CAN FIND BETTER ONES**\n",
    "\n",
    "+ [Machine Learning Mastery](https://machinelearningmastery.com/machine-learning-in-r-step-by-step/)\n",
    "+ [Master in Data Science](https://www.mastersindatascience.org/learning/machine-learning-algorithms/decision-tree/)\n",
    "+ [IBM - What is Machine Learning](https://www.ibm.com/topics/machine-learning)\n",
    "+ Machine Learning by Mueller, J. P. (2021). Machine learning for dummies. John Wiley &amp; Sons. \n",
    "\n",
    "## Training Supervised Machine Learning Models\n",
    "\n",
    "In supervised machine learning, algorithms need to be trained before they can be used to predict data. This involves selecting a smaller portion of data, known as training data, so that the model will learn how to predict the outcome as accurately as possible. The process of training an algorithm is essential for enabling it to learn and improve over time, allowing it to make more accurate predictions and better adapt to new and changing circumstances. Ultimately, the effectiveness of a machine learning model depends on the quality and relevance of its training data.\n",
    "\n",
    "Let's imagine you're interested in predicting an animal's species (either a cat or a dog) based on a dataset that contains variables regarding weight, height, coat color, ear shape, etc. These data can be divided into a training set and a test set. The **training set** is a subset of the data that the model will learn from to make associations between the predictor variables (ie. height, weight etc.) and the outcome (ie. cat or dog). The **test set** is used used to evaluate what the model has learned from the training set. \n",
    "\n",
    "It's typical to split the entire dataset into the training set that contains 60% of the data and the test set that contains 40% of the data:\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/a8c723ec-e50c-4cb9-aff5-4230d0d6fc2c\" width=\"684\"/>\n",
    "\n",
    "*Created with BioRender.com*\n",
    "\n",
    "\n",
    "The process of developing a model involves dividing the data into three main sets:\n",
    "\n",
    "1. **Training Set:** a subset of the data that the model uses to learn from the data by identifying patterns.\n",
    "\n",
    "2. **Validation Set**: a subset of data that is used to evaluate the model's fit in an unbiased way by fine-tuning its parameters and optimizing its performance. This is akin to pop quizzes that help students improve their understanding and performance. \n",
    "\n",
    "2. **Test Set:** a subset of data that is used to evaluate the final model's fit based on the training and validation sets. This is the model's final exam, as it provides an objective assessment of the model's ability to generalize to new, unseen data. \n",
    "\n",
    "It's important to note that the test set should only be used once, after the model has been trained using the training dataset. Using the test set multiple times during the development process can lead to overfitting, where the model performs well on the test data but poorly on new, unseen data. The ideal algorithm is generalizable or flexible enough to accurately predict unseen data. This is known as the bias-variance tradeoff. \n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "The last topic we should mention in this section is **cross validation** or ***k*-fold cross validation**. If the dataset is based on this 60:40 split that we mentioned earlier, our model's accuracy will likely be influenced based upon *where* this 60:40 split occurs in the dataset. Cross validation (CV) is implemented to fine tune a model's parameters and ensure that the model is exposed to more patterns in the dataset, which reduces bias and improves prediction accuracy. \n",
    "\n",
    "It works by equally splitting the samples in the dataset into *k* number of folds or groups. For example, if 5-fold CV were to be run, we would have 5 different groups in the dataset with 4 retained for validation sets to train the model and 1 retained for testing to fine tune the model's parameters. Across the five iterations, each fold would have a chance to be the test set as seen in the figure below. To measure prediction in Random Forest, out of bag (OOB) errors are calculated each time when tuning parameters. The lower the OOB error, the better the model performance.\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/ffe9f044-3c13-4b56-80e1-109380f4a6d9\" width=\"684\"/>\n",
    "\n",
    "*Created with BioRender.com*\n",
    "\n",
    "**LAUREN - CAN YOU PLEASE MAKE SURE THIS ILLUSTRATION ACTUALLY VISUALIZES WHAT'S IN THE CODE BELOW? I MIGHT BE CONFUSING MYSELF.**\n",
    "\n",
    "Confusion matrix metrics would be calculated after each iteration and averaged for the final results. For additional information on the bias-variance tradeoff and the different types of cross validation check out the resources below. \n",
    "\n",
    "### Resources\n",
    "+ [Understanding the Bias-Variance Tradeoff](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n",
    "+ [Cross Validation in Machine Learning](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f)\n",
    "+ [Cross Validation Pros & Cons](https://www.geeksforgeeks.org/cross-validation-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a2a67",
   "metadata": {},
   "source": [
    "## Assessing Model Performance  \n",
    "Metrics from a confusion matrix are used to evalute model performance for classification-based supervised machine learning models. A confusion matrix consists of table that displays the numbers of how often the algorithm correctly or incorrectly predicted the outcome. \n",
    "\n",
    "Going back to our animal classification example, let's say the confusion matrix below is a result of how well the model was able to predict whether an animal was considered to be a cat or a dog.\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/4c7a90a0-725b-48b8-ada4-77a5907131a0\" width=\"684\" />\n",
    "\n",
    "*Created with BioRender.com*\n",
    "\n",
    "Some of the metrics that can be obtained from a confusion matrix are listed below:\n",
    "\n",
    "+ **Balanced Accuracy:** is the ratio of correct predictions (TP + TN) to the total number of predictions (TP + TN + TN + FN) and is typically used to assess overall model performance. This metric is prone to skew for imbalanced data. For example, if the animal dataset had 11 cats and 74 dogs the data would be considered to be imbalanced.\n",
    "\n",
    "+ **Sensitivity or Recall:** evaluates how well the model was able to predict the \"positive\" class. It's the ratio of correctly classified true positives to total number of all true positives (TP + FN). \n",
    "\n",
    "+ **Specificity:** evaluates how well the model was able to predict the \"negative\" class. It's the ratio of correctly classified true negatives to total number of all true negatives (TN + FP). \n",
    "\n",
    "+  **Positive Predictive Value (PPV) or Precision:**  evaluates how well the model was able to predict the \"positive\" class. It's the ratio of correctly classified true positives to total number of predicted positives (TP + FP).\n",
    "\n",
    "+  **Negative Predictive Value (PPV):**  evaluates how well the model was able to predict the \"negative\" class. It's the ratio of correctly classified true negatives to total number of predicted positives (TN + FN).\n",
    "\n",
    "For all metrics, the values fall between 0 and 1, where 0 represents the model not being able to classify any data points correctly and 1 representing that the model was able to classify all test data correctly. Typically a balanced accuracy of at least 0.7 is considered respectable.\n",
    "\n",
    "\n",
    "**Note**: For multiclass classification (more than two labeled outcomes to be predicted), the same metrics are used, but are obtained in a slightly different way. Regression based supervised machine learning models use loss functions to evaluate model performance. For more information regarding confusion matrices and loss functions for regression-based models check out the resources below.\n",
    "\n",
    "### Resources\n",
    " + [Additional Confusion Matrix Metrics](https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5)\n",
    " + [Precision vs. Recall or Specificity vs. Sensitivity](https://towardsdatascience.com/should-i-look-at-precision-recall-or-specificity-sensitivity-3946158aace1)\n",
    " + [Loss Functions for Machine Learning Regression](https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e82dd",
   "metadata": {},
   "source": [
    "## Types of Supervised Machine Learning Algorithms\n",
    "\n",
    "Although we'll be focusing on a random forest model in the coding example below, we want to mention other popular algorithims that can be used for supervised machine learning. These include: \n",
    "\n",
    "+ **K-Nearest Neighbors (KNN):** Uses Euclidean distance to classify a data point in the test set based upon the most common class of neighboring data points.\n",
    "<img src=\"https://user-images.githubusercontent.com/96756991/232493057-1e7ce98b-6985-44cd-98a9-3cfea5994659.png\" width=\"684\" />\n",
    "\n",
    "*Created with BioRender.com*\n",
    "\n",
    "+ **Support Vector Machine (SVM):** Creates a decision boundary line (hyperplane) in n-dimensional space to seperate the data into each class so that when new data is presented they can be easily cateogrized. \n",
    "<img src=\"https://user-images.githubusercontent.com/96756991/233735220-08682ea4-fe13-41c8-8ac5-9ddda7859328.png\" width=\"684\" />\n",
    "\n",
    "*Created with BioRender.com* \n",
    "\n",
    "+ **Random Forest (RF):** Uses a multitude of decison trees trained on a subset of different samples from the training set and the resulting classification of a data point in the test set is aggregated from all the decision trees. A **decision tree** is a hierarchical model that selects the best predictors to classify the data with each node representing a test on a predictor, each branch representing the outcome, and leaf nodes representing the class label. \n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1011_Emission-Mixtures/assets/69641855/690d8e90-059e-47d0-98c0-bd0fc6fbebc3\" width=\"684\" />\n",
    "\n",
    "([Navlani, 2023](https://www.datacamp.com/tutorial/decision-tree-classification-python))\n",
    "\n",
    "### Resources\n",
    "+ [K-Nearst Neighbor](https://www.ibm.com/topics/knn)\n",
    "+ [Support Vector Machine](https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm) **I LIKE THE TEXT IN THIS REFERENCE, BUT IT CONTAINS PYTHON CODE. NOT SURE IF THAT'S CONFUSING.**\n",
    "+ [Random Forest](https://www.ibm.com/in-en/topics/random-forest)\n",
    "+ [Decision Trees](https://www.ibm.com/topics/decision-trees#:~:text=A%20decision%20tree%20is%20a,internal%20nodes%20and%20leaf%20nodes.)\n",
    "\n",
    "Now that we have a better understanding of supervised machine learning, let's go ahead and write the code to predict iAs detection. \n",
    "\n",
    "### Changing Data Types \n",
    "First, `Detect_Concentration` needs to be converted from a character to a factor, so that Random Forest to know that non-detect (binarized as 0) data is considered baseline. `Water_Sample_Date` needs to be converted from a character to a date type, so that Random Forest understands this column contains dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acf23334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 8</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Tax_ID</th><th scope=col>Water_Sample_Date</th><th scope=col>Casing_Depth</th><th scope=col>Well_Depth</th><th scope=col>Static_Water_Depth</th><th scope=col>Flow_Rate</th><th scope=col>pH</th><th scope=col>Detect_Concentration</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;date&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1006004</td><td>2012-09-24</td><td>52</td><td>165</td><td>41</td><td>60.0</td><td>7.7</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1024009</td><td>2015-12-17</td><td>40</td><td>445</td><td>42</td><td> 2.0</td><td>7.3</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>1054019</td><td>2015-02-02</td><td>45</td><td>160</td><td>40</td><td>40.0</td><td>7.4</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1057017</td><td>2012-10-22</td><td>42</td><td>440</td><td>57</td><td> 1.5</td><td>8.0</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>1060006</td><td>2011-01-03</td><td>48</td><td>120</td><td>42</td><td>25.0</td><td>7.1</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>1066006</td><td>2015-12-15</td><td>60</td><td>280</td><td>32</td><td>10.0</td><td>8.2</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 8\n",
       "\\begin{tabular}{r|llllllll}\n",
       "  & Tax\\_ID & Water\\_Sample\\_Date & Casing\\_Depth & Well\\_Depth & Static\\_Water\\_Depth & Flow\\_Rate & pH & Detect\\_Concentration\\\\\n",
       "  & <chr> & <date> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t1 & 1006004 & 2012-09-24 & 52 & 165 & 41 & 60.0 & 7.7 & 0\\\\\n",
       "\t2 & 1024009 & 2015-12-17 & 40 & 445 & 42 &  2.0 & 7.3 & 0\\\\\n",
       "\t3 & 1054019 & 2015-02-02 & 45 & 160 & 40 & 40.0 & 7.4 & 0\\\\\n",
       "\t4 & 1057017 & 2012-10-22 & 42 & 440 & 57 &  1.5 & 8.0 & 1\\\\\n",
       "\t5 & 1060006 & 2011-01-03 & 48 & 120 & 42 & 25.0 & 7.1 & 0\\\\\n",
       "\t6 & 1066006 & 2015-12-15 & 60 & 280 & 32 & 10.0 & 8.2 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 8\n",
       "\n",
       "| <!--/--> | Tax_ID &lt;chr&gt; | Water_Sample_Date &lt;date&gt; | Casing_Depth &lt;dbl&gt; | Well_Depth &lt;dbl&gt; | Static_Water_Depth &lt;dbl&gt; | Flow_Rate &lt;dbl&gt; | pH &lt;dbl&gt; | Detect_Concentration &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 1006004 | 2012-09-24 | 52 | 165 | 41 | 60.0 | 7.7 | 0 |\n",
       "| 2 | 1024009 | 2015-12-17 | 40 | 445 | 42 |  2.0 | 7.3 | 0 |\n",
       "| 3 | 1054019 | 2015-02-02 | 45 | 160 | 40 | 40.0 | 7.4 | 0 |\n",
       "| 4 | 1057017 | 2012-10-22 | 42 | 440 | 57 |  1.5 | 8.0 | 1 |\n",
       "| 5 | 1060006 | 2011-01-03 | 48 | 120 | 42 | 25.0 | 7.1 | 0 |\n",
       "| 6 | 1066006 | 2015-12-15 | 60 | 280 | 32 | 10.0 | 8.2 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "  Tax_ID  Water_Sample_Date Casing_Depth Well_Depth Static_Water_Depth\n",
       "1 1006004 2012-09-24        52           165        41                \n",
       "2 1024009 2015-12-17        40           445        42                \n",
       "3 1054019 2015-02-02        45           160        40                \n",
       "4 1057017 2012-10-22        42           440        57                \n",
       "5 1060006 2011-01-03        48           120        42                \n",
       "6 1066006 2015-12-15        60           280        32                \n",
       "  Flow_Rate pH  Detect_Concentration\n",
       "1 60.0      7.7 0                   \n",
       "2  2.0      7.3 0                   \n",
       "3 40.0      7.4 0                   \n",
       "4  1.5      8.0 1                   \n",
       "5 25.0      7.1 0                   \n",
       "6 10.0      8.2 1                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arsenic_data = arsenic_data %>%\n",
    "    # Converting `Detect_Concentration from a character to a factor\n",
    "    mutate(Detect_Concentration = relevel(factor(ifelse(Detect_Concentration == \"D\", 1, 0)), ref = \"0\"),\n",
    "        # converting water sample date from a character to a date type \n",
    "        Water_Sample_Date = mdy(Water_Sample_Date))\n",
    "\n",
    "head(arsenic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be60453a",
   "metadata": {},
   "source": [
    "### Testing for differences in predictor variables acrosss the outcome classes\n",
    "\n",
    "It's useful to run summary statistics on the variables that will be used as predictors in the model to see if there are differences in distributions between the outcomes classes (either non-detect or detect in this case). Typically, greater signficance often leads to better predictivity, since the model is better able to separate the classes. We'll use the `tbl_summary()` function from the `gtsummary` package.\n",
    "\n",
    "For more information on the `tbl_summary()` function, check out this helpful [Tutorial](https://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3538a5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>**Characteristic**</th><th scope=col>**N**</th><th scope=col>**0**, N = 515</th><th scope=col>**1**, N = 198</th><th scope=col>**p-value**</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>Water_Sample_Date </td><td>713</td><td>2013-06-05 (979.174260670888)</td><td>2013-03-05 (957.843005291701)</td><td>0.3   </td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>Casing_Depth      </span></td><td>713</td><td><span style=white-space:pre-wrap>74 (33)                      </span></td><td><span style=white-space:pre-wrap>55 (23)                      </span></td><td>&lt;0.001</td></tr>\n",
       "\t<tr><td>Well_Depth        </td><td>713</td><td>301 (144)                    </td><td>334 (128)                    </td><td>0.005 </td></tr>\n",
       "\t<tr><td>Static_Water_Depth</td><td>713</td><td>35 (12)                      </td><td>36 (13)                      </td><td>0.4   </td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>Flow_Rate         </span></td><td>713</td><td><span style=white-space:pre-wrap>25 (33)                      </span></td><td><span style=white-space:pre-wrap>14 (16)                      </span></td><td>&lt;0.001</td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>pH                </span></td><td>713</td><td><span style=white-space:pre-wrap>7.45 (0.55)                  </span></td><td><span style=white-space:pre-wrap>7.82 (0.40)                  </span></td><td>&lt;0.001</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " **Characteristic** & **N** & **0**, N = 515 & **1**, N = 198 & **p-value**\\\\\n",
       " <chr> & <chr> & <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t Water\\_Sample\\_Date  & 713 & 2013-06-05 (979.174260670888) & 2013-03-05 (957.843005291701) & 0.3   \\\\\n",
       "\t Casing\\_Depth       & 713 & 74 (33)                       & 55 (23)                       & <0.001\\\\\n",
       "\t Well\\_Depth         & 713 & 301 (144)                     & 334 (128)                     & 0.005 \\\\\n",
       "\t Static\\_Water\\_Depth & 713 & 35 (12)                       & 36 (13)                       & 0.4   \\\\\n",
       "\t Flow\\_Rate          & 713 & 25 (33)                       & 14 (16)                       & <0.001\\\\\n",
       "\t pH                 & 713 & 7.45 (0.55)                   & 7.82 (0.40)                   & <0.001\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 5\n",
       "\n",
       "| **Characteristic** &lt;chr&gt; | **N** &lt;chr&gt; | **0**, N = 515 &lt;chr&gt; | **1**, N = 198 &lt;chr&gt; | **p-value** &lt;chr&gt; |\n",
       "|---|---|---|---|---|\n",
       "| Water_Sample_Date  | 713 | 2013-06-05 (979.174260670888) | 2013-03-05 (957.843005291701) | 0.3    |\n",
       "| Casing_Depth       | 713 | 74 (33)                       | 55 (23)                       | &lt;0.001 |\n",
       "| Well_Depth         | 713 | 301 (144)                     | 334 (128)                     | 0.005  |\n",
       "| Static_Water_Depth | 713 | 35 (12)                       | 36 (13)                       | 0.4    |\n",
       "| Flow_Rate          | 713 | 25 (33)                       | 14 (16)                       | &lt;0.001 |\n",
       "| pH                 | 713 | 7.45 (0.55)                   | 7.82 (0.40)                   | &lt;0.001 |\n",
       "\n"
      ],
      "text/plain": [
       "  **Characteristic** **N** **0**, N = 515               \n",
       "1 Water_Sample_Date  713   2013-06-05 (979.174260670888)\n",
       "2 Casing_Depth       713   74 (33)                      \n",
       "3 Well_Depth         713   301 (144)                    \n",
       "4 Static_Water_Depth 713   35 (12)                      \n",
       "5 Flow_Rate          713   25 (33)                      \n",
       "6 pH                 713   7.45 (0.55)                  \n",
       "  **1**, N = 198                **p-value**\n",
       "1 2013-03-05 (957.843005291701) 0.3        \n",
       "2 55 (23)                       <0.001     \n",
       "3 334 (128)                     0.005      \n",
       "4 36 (13)                       0.4        \n",
       "5 14 (16)                       <0.001     \n",
       "6 7.82 (0.40)                   <0.001     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arsenic_data %>%\n",
    "    tbl_summary(by = Detect_Concentration,\n",
    "    # Selecting columns to include\n",
    "    include = colnames(arsenic_data[c(2:8)]), \n",
    "    # Displaying the mean and standard deviation in parantheses for all continuous variables\n",
    "                statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %>%\n",
    "    # Adding a column that displays the total number of samples for each variable\n",
    "    # This will be 713 for all variables since we have no missing data\n",
    "    add_n() %>% \n",
    "    # Adding a column that dispalys the p value from anova\n",
    "    add_p(test = list(all_continuous() ~ \"aov\")) %>% \n",
    "    as_tibble()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38577cf1",
   "metadata": {},
   "source": [
    "All the variables are very signficant with the exception of the sample date and the static water depth, therefore the model should predict fairly well.\n",
    "\n",
    "## Setting up Cross Validation\n",
    "\n",
    "As mentioned above, cross validation is done so that the model is trained and tested on different portions of the entire dataset. We'll use 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64cff629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the splits in the dataset are random, a seed is set for reproducibility to ensure the splits are occuring\n",
    "# in the same locations each time the code is run\n",
    "set.seed(12)\n",
    "\n",
    "# 5-fold cross validation\n",
    "# Saving the index (row number) where the 5 splits are occuring\n",
    "# These indices will be iterated through using a loop to create each training and testing datasets\n",
    "arsenic_index = createFolds(arsenic_data$Detect_Concentration, k = 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d2dd6f",
   "metadata": {},
   "source": [
    "Within CV, different parameters will be tested, including the number of trees \"grown\" by RF (`ntree_values`) and the number of predictors used in those trees (`mtry_values`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9dfef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntree_values = c(50, 250, 500) # number of trees \n",
    "p = dim(well_data)[2] - 1 # number of predictor variables in dataset\n",
    "mtry_values = c(sqrt(p), p/2, p) # number of predictors to be used in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616dc50",
   "metadata": {},
   "source": [
    "## Predicting iAs Detection with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72462a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 1 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Balanced Accuracy</th><th scope=col>Sensitivity</th><th scope=col>Specificity</th><th scope=col>PPV</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.6546658</td><td>0.4588462</td><td>0.8504854</td><td>0.5401044</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 × 4\n",
       "\\begin{tabular}{llll}\n",
       " Balanced Accuracy & Sensitivity & Specificity & PPV\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.6546658 & 0.4588462 & 0.8504854 & 0.5401044\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 × 4\n",
       "\n",
       "| Balanced Accuracy &lt;dbl&gt; | Sensitivity &lt;dbl&gt; | Specificity &lt;dbl&gt; | PPV &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| 0.6546658 | 0.4588462 | 0.8504854 | 0.5401044 |\n",
       "\n"
      ],
      "text/plain": [
       "  Balanced Accuracy Sensitivity Specificity PPV      \n",
       "1 0.6546658         0.4588462   0.8504854   0.5401044"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting the seed again so the predictions are consistent\n",
    "set.seed(12)\n",
    "\n",
    "# Creating an empty dataframe to save the metrics\n",
    "metrics_df = data.frame()\n",
    "\n",
    "# Iterating through the cross validation folds\n",
    "for (i in 1:length(arsenic_index)){\n",
    "    # Training data\n",
    "    data_train = arsenic_data[-arsenic_index[[i]],]\n",
    "    \n",
    "    # Test data\n",
    "    data_test = arsenic_data[arsenic_index[[i]],]\n",
    "    \n",
    "    # Creating empty lists and dataframes to store errors \n",
    "    reg_rf_pred_tune = list()\n",
    "    rf_OOB_errors = list()\n",
    "    rf_error_df = data.frame()\n",
    "    \n",
    "    # Using ntree and mtry values to determine which combination yields the smallest OOB error from the \n",
    "    # validation datasets\n",
    "    for (j in 1:length(ntree_values)){\n",
    "        for (k in 1:length(mtry_values)){\n",
    "            \n",
    "            # Running RF to tune parameters\n",
    "            reg_rf_pred_tune[[k]] = randomForest(Detect_Concentration ~ ., data = data_train, \n",
    "                                                 ntree = ntree_values[j], mtry = mtry_values[k])\n",
    "            # Obtaining the OOB error\n",
    "            rf_OOB_errors[[k]] = data.frame(\"Tree Number\" = ntree_values[j], \"Variable Number\" = mtry_values[k], \n",
    "                                   \"OOB_errors\" = reg_rf_pred_tune[[k]]$err.rate[ntree_values[j],1])\n",
    "            \n",
    "            # Storing the values in a dataframe\n",
    "            rf_error_df = rbind(rf_error_df, rf_OOB_errors[[k]])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Finding the lowest OOB error using best number of predictors at split\n",
    "    best_oob_errors <- which(rf_error_df$OOB_errors == min(rf_error_df$OOB_errors))\n",
    "\n",
    "    # Now running RF on the entire training set with the tuned parameters\n",
    "    reg_rf <- randomForest(Detect_Concentration ~ ., data = data_train,\n",
    "                               ntree = rf_error_df$Tree.Number[min(best_oob_errors)],\n",
    "                               mtry = rf_error_df$Variable.Number[min(best_oob_errors)])\n",
    "\n",
    "    # Predicting on test set and adding the predicted values as an additional column to the test data\n",
    "    data_test$Pred_Detect_Concentration = predict(reg_rf, newdata = data_test, type = \"response\")\n",
    "\n",
    "    # Obtaining the confusion matrix\n",
    "    matrix = confusionMatrix(data = data_test$Pred_Detect_Concentration, \n",
    "                             reference = data_test$Detect_Concentration, positive = \"1\")\n",
    "    \n",
    "    # Extracting balanced accuracy, sensitivity, specificity, and PPV\n",
    "    matrix_values = data.frame(t(c(matrix$byClass[11])), t(c(matrix$byClass[1:3])))\n",
    "    \n",
    "    # Adding values to df to be averaged\n",
    "    metrics_df = rbind(metrics_df, matrix_values)\n",
    "}\n",
    "\n",
    "# Taking average\n",
    "metrics_df = metrics_df %>%\n",
    "    summarise(`Balanced Accuracy` = mean(Balanced.Accuracy), Sensitivity = mean(Sensitivity), \n",
    "          Specificity = mean(Specificity), PPV = mean(Pos.Pred.Value))\n",
    "\n",
    "# Viewing the model performance metrics\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cc5b0",
   "metadata": {},
   "source": [
    "Takeaways from this confusion matrix:\n",
    "\n",
    "+ Overall, the model performed moderately well at predicting if iAs would be detected based on a balanced accuracy of ~0.7\n",
    "+ RF did a poor job of predicting detect data with a sensitivity and a PPV of ~0.5\n",
    "+ The model was significantly better at predicting non-detect data based on a specificity of ~0.9 \n",
    "\n",
    "**I THINK WE COULD DISCUSS THE CLASS IMBALANCE ISSUE AND ADD EITHER SMOTE OR AUC, BUT THE MODEL PERFORMED DECENTLY WELL SO I'M NOT SURE IF THAT'S REALLY NECESSARY.**\n",
    "\n",
    "## Concluding Remarks\n",
    "\n",
    "In conclusion, this training module has provided an informative introduction to supervised machine learning using classification techniques in R. Machine learning is a powerful tool that can help researchers gain new insights and improve models to analyze complex datasets faster and in a more comprehensive way. The example we've explored demonstrates the utility of supervised machine learning models on datasets with a plethora of features.\n",
    "\n",
    "\n",
    "## Test Your Knowledge \n",
    "\n",
    "1. Using the \"Chromium_Data\", use RF to determine if well water data can be accurate predictors of Chromium detection.\n",
    "**I FIGURED IT'D BE EASIEST TO MAKE THIS DATASET THAT HAS THE SAME PREDICTORS AS THE ARSENIC DATA, BUT WE CAN ALSO CHALLENGE THEM MORE WITH A DATASET WITH A DIFFERENT STRUCTURE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae87403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
