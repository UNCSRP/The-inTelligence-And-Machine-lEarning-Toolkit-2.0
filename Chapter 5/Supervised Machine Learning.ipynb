{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc458e6f",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning \n",
    "\n",
    "This training module was developed by Alexis Payton, MS, Oyemwenosa N. Avenbuan, and Dr. Julia E. Rager\n",
    "\n",
    "Fall 2023\n",
    "\n",
    "## Introduction to Machine Learning, including Supervised Machine Learning\n",
    "Machine learning is a field that has been around for decades, but has exploded in popularity and utility in recent years due to proliferation of big data. Through the building of models, machine learning has the ability to sift through and learn from large volumes of data and use that knowledge to solve problems. The challenges of big and high dimensional data as they pertain to environmental health and how machine learning can mitigate some of those challenges are discussed further in [Payton et. al](https://www.frontiersin.org/articles/10.3389/ftox.2023.1171175/full).\n",
    "\n",
    "## Types of Machine Learning\n",
    "Within the field of machine learning, there are many different types of machine learning that can be run to address environmental health research questions. The two most commonly used categories used in environmental health research are: (1) supervised machine learning and (2) unsupervised machine learning.\n",
    "\n",
    "**Supervised machine learning** involves training a model using a labeled dataset, where each dependent or predictor variable is associated with an independent variable with a known outcome. This allows the model to learn how to predict the labeled outcome on data it hasn't \"seen\" before based on the patterns and relationships it previously identified in the data. For example, supervised machine learning has been used for cancer prediction and prognosis based on variables like tumor size, stage, and age ([Lynch et. al](https://www.sciencedirect.com/science/article/abs/pii/S1386505617302368?via%3Dihub), [Asadi et. al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7416093/)). \n",
    "\n",
    "Supervised machine learning includes: \n",
    "+ Classification: Using algorithms to classify a categorical outcome (ie. plant species, disease status, etc.)\n",
    "+ Regression: Using algorithms to predict a continuous outcome (ie. temperature, chemical concentration, etc.)\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/3c22a8fc-ada5-4199-9967-77f504d99d2a\" width=\"684\" />\n",
    "\n",
    "([Soni, 2018](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d))\n",
    "\n",
    "\n",
    "**Unsupervised machine learning**, on the other hand, involves using models to find patterns or associations between variables in dataset that lack a known or labeled outcome. For example, unsupervised machine learning has been used to find associations of co-expressed genes within various biological pathways ([Botía et. al](https://bmcsystbiol.biomedcentral.com/articles/10.1186/s12918-017-0420-6), [Pagnuco et. al](https://www.sciencedirect.com/science/article/pii/S0888754317300575?via%3Dihub)).\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/4f78adef-97b6-425f-9a51-43315b0fb7b2\" width=\"684\" />\n",
    "\n",
    "([Langs et. al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6244522/))\n",
    "\n",
    "Overall, the distinction between supervised and unsupervised learning is an important concept in machine learning, as it can inform the choice of algorithms and techniques used to analyze and make predictions from data. It is worth noting that there are also other types of machine learning, such as semi-supervised learning, reinforcement learning and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bc9b19",
   "metadata": {},
   "source": [
    "## Training Supervised Machine Learning Models\n",
    "\n",
    "In supervised machine learning, algorithms need to be trained before they can be used to predict data. This involves selecting a smaller portion of data, known as training data, so that the model will learn how to predict the outcome as accurately as possible. The process of training an algorithm is essential for enabling it to learn and improve over time, allowing it to make more accurate predictions and better adapt to new and changing circumstances. Ultimately, the effectiveness of a machine learning model depends on the quality and relevance of its training data.\n",
    "\n",
    "Let's imagine you're interested in predicting an animal's species (either a cat or a dog) based on a dataset that contains variables regarding weight, height, coat color, ear shape, etc. These data can be divided into a training set and a test set. The **training set** is a subset of the data that the model will learn from to make associations between the predictor variables (ie. height, weight etc.) and the outcome (ie. cat or dog). The **test set** is used used to evaluate what the model has learned from the training set. \n",
    "\n",
    "It is common to split the entire dataset into the training set that contains 60% of the data and the test set that contains 40% of the data:\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/a8c723ec-e50c-4cb9-aff5-4230d0d6fc2c\" width=\"684\"/>\n",
    "\n",
    "*Created with BioRender.com*\n",
    "\n",
    "Other common splits include 70% training / 30% test and 80% training / 20% test.\n",
    "\n",
    "The process of developing a model often involves dividing the data into three main sets:\n",
    "\n",
    "1. **Training Set:** a subset of the data that the model uses to learn from the data by identifying patterns.\n",
    "\n",
    "2. **Validation Set**: a subset of training set data that is used to evaluate the model's fit in an unbiased way by fine-tuning its parameters and optimizing its performance. This is akin to pop quizzes that help students improve their understanding and performance. \n",
    "\n",
    "2. **Test Set:** a subset of data that is used to evaluate the final model's fit based on the training and validation sets. This is akin to the model's final exam, as it provides an objective assessment of the model's ability to generalize to new, unseen data. \n",
    "\n",
    "It is important to note that the test set should only be used once, after the model has been trained using the training dataset. Using the test set multiple times during the development process can lead to overfitting, where the model performs well on the test data but poorly on new, unseen data. The ideal algorithm is generalizable or flexible enough to accurately predict unseen data. This is known as the bias-variance tradeoff. For further information on the bias-variance tradeoff, see [Understanding the Bias-Variance Tradeoff](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229).\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "The last topic we should mention in this section is **cross validation** or ***k*-fold cross validation**. If the dataset is based on this 60:40 split that we mentioned earlier, our model's accuracy will likely be influenced based upon *where* this 60:40 split occurs in the dataset. This will likely bias the data and reduce the algorithm's ability to predict accurately. Cross validation (CV) is implemented to fine tune a model's parameters and ensure that the model is exposed to more patterns in the dataset, which reduces bias and improves prediction accuracy. \n",
    "\n",
    "It works by equally splitting the samples in the dataset into *k* number of folds or groups. For example, if 5-fold CV were to be run, we would have 5 different groups in the dataset with 4 retained for validation sets to train the model and 1 retained for testing to fine tune the model's parameters. Across the five iterations, each fold would have a chance to be the test set as seen in the figure below. To measure prediction in Random Forest, out of bag (OOB) errors are calculated each time when tuning parameters. The lower the OOB error, the better the model performance.\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/ffe9f044-3c13-4b56-80e1-109380f4a6d9\" width=\"684\"/>\n",
    "\n",
    "*Created with BioRender.com*\n",
    "\n",
    "**LAUREN - CAN YOU PLEASE MAKE SURE THIS ILLUSTRATION ACTUALLY VISUALIZES WHAT'S IN THE CODE BELOW? I MIGHT BE CONFUSING MYSELF.**\n",
    "\n",
    "Confusion matrix metrics would be calculated after each iteration and averaged for the final results. Check out these resources for additional information on [Cross Validation in Machine Learning](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f) and [Cross Validation Pros & Cons](https://www.geeksforgeeks.org/cross-validation-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a2a67",
   "metadata": {},
   "source": [
    "## Assessing Model Performance  \n",
    "Metrics from a confusion matrix are used to measure model performance for classification-based supervised machine learning models. A confusion matrix consists of table that displays the numbers of how often the algorithm correctly or incorrectly predicted the outcome. \n",
    "\n",
    "Going back to our animal classification example, let's say the confusion matrix below is a result of how well the model was able to predict whether an animal was considered to be a cat or a dog.\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/4c7a90a0-725b-48b8-ada4-77a5907131a0\" width=\"684\" />\n",
    "\n",
    "*Created with BioRender.com*\n",
    "\n",
    "Some of the metrics that can be obtained from a confusion matrix are listed below:\n",
    "\n",
    "+ **Balanced Accuracy:** is the ratio of correct predictions (TP + TN) to the total number of predictions (TP + TN + TN + FN) and is typically used to assess overall model performance. This metric is prone to skew for imbalanced data. For example, if the animal dataset had 11 cats and 74 dogs the data would be considered to be imbalanced.  \n",
    "\n",
    "+ **Sensitivity or Recall:** evaluates how well the model was able to predict the \"positive\" class. It's the ratio of correctly classified true positives to total number of all true positives (TP + FN). \n",
    "\n",
    "+ **Specificity:** evaluates how well the model was able to predict the \"negative\" class. It's the ratio of correctly classified true negatives to total number of all true negatives (TN + FP). \n",
    "\n",
    "+  **Positive Predictive Value (PPV) or Precision:**  evaluates how well the model was able to predict the \"positive\" class. It's the ratio of correctly classified true positives to total number of predicted positives (TP + FP).\n",
    "\n",
    "+  **Negative Predictive Value (NPV):**  evaluates how well the model was able to predict the \"negative\" class. It's the ratio of correctly classified true negatives to total number of predicted positives (TN + FN).\n",
    "\n",
    "For all metrics, the values fall between 0 and 1, where 0 indicates the model was not able to classify any data points correctly and 1 indicating that the model was able to classify all test data correctly. Although subjective, a balanced accuracy of at least 0.7 is considered respectable ([Barkved, 2022](https://www.obviously.ai/post/machine-learning-model-performance#:~:text=Good%20accuracy%20in%20machine%20learning,also%20consistent%20with%20industry%20standards.)).\n",
    "\n",
    "\n",
    "**Note**: For multi-class classification (more than two labeled outcomes to be predicted), the same metrics are used, but are obtained in a slightly different way. Regression based supervised machine learning models use loss functions to evaluate model performance. For more information regarding confusion matrices and loss functions for regression-based models, see:\n",
    "\n",
    " + [Additional Confusion Matrix Metrics](https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5)\n",
    " + [Precision vs. Recall or Specificity vs. Sensitivity](https://towardsdatascience.com/should-i-look-at-precision-recall-or-specificity-sensitivity-3946158aace1)\n",
    " + [Loss Functions for Machine Learning Regression](https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a067f",
   "metadata": {},
   "source": [
    "## Types of Supervised Machine Learning Algorithms\n",
    "\n",
    "Although this module's example focuses random forest model in the coding example below, other commonly used algorithims for supervised machine learning include: \n",
    "\n",
    "+ **K-Nearest Neighbors (KNN):** Uses Euclidean distance to classify a data point in the test set based upon the most common class of neighboring data points. For more information on KNN, see [K-Nearst Neighbor](https://www.ibm.com/topics/knn)\n",
    "<img src=\"https://user-images.githubusercontent.com/96756991/232493057-1e7ce98b-6985-44cd-98a9-3cfea5994659.png\" width=\"684\" />\n",
    "\n",
    "*Created with BioRender.com*\n",
    "\n",
    "+ **Support Vector Machine (SVM):** Creates a decision boundary line (hyperplane) in n-dimensional space to seperate the data into each class so that when new data is presented they can be easily cateogrized. For more information on SVM, see [Support Vector Machine](https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm).\n",
    "<img src=\"https://user-images.githubusercontent.com/96756991/233735220-08682ea4-fe13-41c8-8ac5-9ddda7859328.png\" width=\"684\" />\n",
    "\n",
    "*Created with BioRender.com* \n",
    "\n",
    "+ **Random Forest (RF):** Uses a multitude of decison trees trained on a subset of different samples from the training set and the resulting classification of a data point in the test set is aggregated from all the decision trees. A **decision tree** is a hierarchical model that selects the best predictors to classify the data with each node representing a test on a predictor, each branch representing the outcome, and leaf nodes representing the class label. For more information on RF and decision trees, check out [Random Forest](https://www.ibm.com/in-en/topics/random-forest) and\n",
    "[Decision Trees](https://www.ibm.com/topics/decision-trees#:~:text=A%20decision%20tree%20is%20a,internal%20nodes%20and%20leaf%20nodes.).\n",
    "\n",
    "Here is an illustration of the terminology used to describe the underlying structure of a decision tree:\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1011_Emission-Mixtures/assets/69641855/f4d15d7c-e2ee-42d0-b58c-4c8d3240f6be\" width=\"684\" />\n",
    "\n",
    "\n",
    "Here is a published example decision tree with potential variables and decisions informing low vs high risk of having a heart attack: **I'LL REMAKE THIS SO IT'S BASED ON THE SAME EXAMPLE FROM ABOVE FOR TAME**\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1011_Emission-Mixtures/assets/69641855/690d8e90-059e-47d0-98c0-bd0fc6fbebc3\" width=\"684\" />\n",
    "\n",
    "([Navlani, 2023](https://www.datacamp.com/tutorial/decision-tree-classification-python))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c55b6df",
   "metadata": {},
   "source": [
    "## Introduction to Training Module\n",
    "In this activity, we will analyze an example dataset to see whether we can use environmental monitoring information to predict areas of contamination through random forest modeling. Specifically, RF will leverage a dataset of well water variables that span geospatial location, sampling date, and well water attributes, with the goal of predicting whether detectable levels of inorganic arsenic (iAs) are present. This dataset was obtained through the sampling of 713 private wells across North Carolina using a method that was capable of detecting levels of iAs. After the algorithm has been trained and tested, model performance is assessed using the aforementioned confusion matrix metrics.\n",
    "\n",
    "\n",
    "## Training Module's Environmental Health Questions\n",
    "\n",
    "This training module was specifically developed to answer the following environmental health questions:\n",
    "\n",
    "1. Which well water predictor variables significantly differ between samples containing detectable levels of iAs vs samples that have non-detect levels of iAs?\n",
    "\n",
    "2. How can we build and evaluate the performance of this RF model?\n",
    "\n",
    "## Script Preparations\n",
    "\n",
    "### Cleaning the global environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0f31c-c7b7-4026-b841-3bea6d84612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list=ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7d069",
   "metadata": {},
   "source": [
    "### Installing required R packages\n",
    "If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (!requireNamespace(\"readxl\"))\n",
    "  install.packages(\"readxl\");\n",
    "if (!requireNamespace(\"lubridate\"))\n",
    "  install.packages(\"lubridate\");\n",
    "if (!requireNamespace(\"tidyverse\"))\n",
    "  install.packages(\"tidyverse\");\n",
    "if (!requireNamespace(\"gtsummary\"))\n",
    "  install.packages(\"gtsummary\");\n",
    "if (!requireNamespace(\"flextable\"))\n",
    "  install.packages(\"flextable\");\n",
    "if (!requireNamespace(\"caret\"))\n",
    "  install.packages(\"caret\");\n",
    "if (!requireNamespace(\"randomForest\"))\n",
    "  install.packages(\"randomForest\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ff895",
   "metadata": {},
   "source": [
    "### Loading R packages required for this session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d30a7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘lubridate’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    date, intersect, setdiff, union\n",
      "\n",
      "\n",
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.1.3     \u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.5.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.4.3     \u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 1.0.2     \u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.3.0\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "\n",
      "Attaching package: ‘flextable’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:gtsummary’:\n",
      "\n",
      "    as_flextable, continuous_summary\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    compose\n",
      "\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "\n",
      "Attaching package: ‘caret’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    lift\n",
      "\n",
      "\n",
      "randomForest 4.7-1.1\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(readxl);\n",
    "library(lubridate);\n",
    "library(tidyverse);\n",
    "library(gtsummary);\n",
    "library(flextable);\n",
    "library(caret);\n",
    "library(randomForest);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5445cf",
   "metadata": {},
   "source": [
    "### Set your working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcf387",
   "metadata": {},
   "outputs": [],
   "source": [
    "setwd(\"/filepath to where your input files are\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4396af",
   "metadata": {},
   "source": [
    "TO DELETE IN FINAL MODULE (NOT NOW):\n",
    "```{r , include=FALSE}\n",
    "setwd(\"/Users/juliarager/Library/CloudStorage/OneDrive-UniversityofNorthCarolinaatChapelHill/CEMALB/CEMALB_DataAnalysisPM/Shared Project Folders/TAME 2.0/3. TAME 2.0 Code & Input Files/Chapter5/Supervised ML/Module5\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0440d6",
   "metadata": {},
   "source": [
    "### Importing example dataset\n",
    "**Will need to change input to module number and add module number to the file itself**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0d612c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 8</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Tax_ID</th><th scope=col>Water_Sample_Date</th><th scope=col>Casing_Depth</th><th scope=col>Well_Depth</th><th scope=col>Static_Water_Depth</th><th scope=col>Flow_Rate</th><th scope=col>pH</th><th scope=col>Detect_Concentration</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1006004</td><td>9/24/12 </td><td>52</td><td>165</td><td>41</td><td>60.0</td><td>7.7</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1024009</td><td>12/17/15</td><td>40</td><td>445</td><td>42</td><td> 2.0</td><td>7.3</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>1054019</td><td>2/2/15  </td><td>45</td><td>160</td><td>40</td><td>40.0</td><td>7.4</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1057017</td><td>10/22/12</td><td>42</td><td>440</td><td>57</td><td> 1.5</td><td>8.0</td><td>D </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>1060006</td><td>1/3/11  </td><td>48</td><td>120</td><td>42</td><td>25.0</td><td>7.1</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>1066006</td><td>12/15/15</td><td>60</td><td>280</td><td>32</td><td>10.0</td><td>8.2</td><td>D </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 8\n",
       "\\begin{tabular}{r|llllllll}\n",
       "  & Tax\\_ID & Water\\_Sample\\_Date & Casing\\_Depth & Well\\_Depth & Static\\_Water\\_Depth & Flow\\_Rate & pH & Detect\\_Concentration\\\\\n",
       "  & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & 1006004 & 9/24/12  & 52 & 165 & 41 & 60.0 & 7.7 & ND\\\\\n",
       "\t2 & 1024009 & 12/17/15 & 40 & 445 & 42 &  2.0 & 7.3 & ND\\\\\n",
       "\t3 & 1054019 & 2/2/15   & 45 & 160 & 40 & 40.0 & 7.4 & ND\\\\\n",
       "\t4 & 1057017 & 10/22/12 & 42 & 440 & 57 &  1.5 & 8.0 & D \\\\\n",
       "\t5 & 1060006 & 1/3/11   & 48 & 120 & 42 & 25.0 & 7.1 & ND\\\\\n",
       "\t6 & 1066006 & 12/15/15 & 60 & 280 & 32 & 10.0 & 8.2 & D \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 8\n",
       "\n",
       "| <!--/--> | Tax_ID &lt;chr&gt; | Water_Sample_Date &lt;chr&gt; | Casing_Depth &lt;dbl&gt; | Well_Depth &lt;dbl&gt; | Static_Water_Depth &lt;dbl&gt; | Flow_Rate &lt;dbl&gt; | pH &lt;dbl&gt; | Detect_Concentration &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 1006004 | 9/24/12  | 52 | 165 | 41 | 60.0 | 7.7 | ND |\n",
       "| 2 | 1024009 | 12/17/15 | 40 | 445 | 42 |  2.0 | 7.3 | ND |\n",
       "| 3 | 1054019 | 2/2/15   | 45 | 160 | 40 | 40.0 | 7.4 | ND |\n",
       "| 4 | 1057017 | 10/22/12 | 42 | 440 | 57 |  1.5 | 8.0 | D  |\n",
       "| 5 | 1060006 | 1/3/11   | 48 | 120 | 42 | 25.0 | 7.1 | ND |\n",
       "| 6 | 1066006 | 12/15/15 | 60 | 280 | 32 | 10.0 | 8.2 | D  |\n",
       "\n"
      ],
      "text/plain": [
       "  Tax_ID  Water_Sample_Date Casing_Depth Well_Depth Static_Water_Depth\n",
       "1 1006004 9/24/12           52           165        41                \n",
       "2 1024009 12/17/15          40           445        42                \n",
       "3 1054019 2/2/15            45           160        40                \n",
       "4 1057017 10/22/12          42           440        57                \n",
       "5 1060006 1/3/11            48           120        42                \n",
       "6 1066006 12/15/15          60           280        32                \n",
       "  Flow_Rate pH  Detect_Concentration\n",
       "1 60.0      7.7 ND                  \n",
       "2  2.0      7.3 ND                  \n",
       "3 40.0      7.4 ND                  \n",
       "4  1.5      8.0 D                   \n",
       "5 25.0      7.1 ND                  \n",
       "6 10.0      8.2 D                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "arsenic_data <- data.frame(read_excel(\"Module5/Module5_Arsenic_Data.xlsx\"))\n",
    "\n",
    "# View the top of the dataset\n",
    "head(arsenic_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc320e75",
   "metadata": {},
   "source": [
    "The columns in this dataset are described below:\n",
    "+ `Tax_ID`: Tax ID for the property\n",
    "+ `Water_Sample_ID`: Date that the well was sampled \n",
    "+ `Casing_Depth`: Depth of the casing of the well (ft)\n",
    "+ `Well_Depth`: Depth of the well (ft)\n",
    "+ `Static_Water_Depth`: Static water depth in the well (ft)\n",
    "+ `Flow_Rate`: Well flow rate (gallons per minute)\n",
    "+ `pH`: pH of water sample\n",
    "+ `Detect_Concentration`: Binary identifier (either non-detect (ND) or detect (D)) if iAs concentration detected in water sample "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e82dd",
   "metadata": {},
   "source": [
    "### Changing Data Types \n",
    "First, `Detect_Concentration` needs to be converted from a character to a factor, so that Random Forest to know that non-detect (binarized as 0) data is considered baseline. `Water_Sample_Date` needs to be converted from a character to a date type, so that Random Forest understands this column contains dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf23334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 7</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Water_Sample_Date</th><th scope=col>Casing_Depth</th><th scope=col>Well_Depth</th><th scope=col>Static_Water_Depth</th><th scope=col>Flow_Rate</th><th scope=col>pH</th><th scope=col>Detect_Concentration</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;date&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>2012-09-24</td><td>52</td><td>165</td><td>41</td><td>60.0</td><td>7.7</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>2015-12-17</td><td>40</td><td>445</td><td>42</td><td> 2.0</td><td>7.3</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>2015-02-02</td><td>45</td><td>160</td><td>40</td><td>40.0</td><td>7.4</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>2012-10-22</td><td>42</td><td>440</td><td>57</td><td> 1.5</td><td>8.0</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>2011-01-03</td><td>48</td><td>120</td><td>42</td><td>25.0</td><td>7.1</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>2015-12-15</td><td>60</td><td>280</td><td>32</td><td>10.0</td><td>8.2</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 7\n",
       "\\begin{tabular}{r|lllllll}\n",
       "  & Water\\_Sample\\_Date & Casing\\_Depth & Well\\_Depth & Static\\_Water\\_Depth & Flow\\_Rate & pH & Detect\\_Concentration\\\\\n",
       "  & <date> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t1 & 2012-09-24 & 52 & 165 & 41 & 60.0 & 7.7 & 0\\\\\n",
       "\t2 & 2015-12-17 & 40 & 445 & 42 &  2.0 & 7.3 & 0\\\\\n",
       "\t3 & 2015-02-02 & 45 & 160 & 40 & 40.0 & 7.4 & 0\\\\\n",
       "\t4 & 2012-10-22 & 42 & 440 & 57 &  1.5 & 8.0 & 1\\\\\n",
       "\t5 & 2011-01-03 & 48 & 120 & 42 & 25.0 & 7.1 & 0\\\\\n",
       "\t6 & 2015-12-15 & 60 & 280 & 32 & 10.0 & 8.2 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 7\n",
       "\n",
       "| <!--/--> | Water_Sample_Date &lt;date&gt; | Casing_Depth &lt;dbl&gt; | Well_Depth &lt;dbl&gt; | Static_Water_Depth &lt;dbl&gt; | Flow_Rate &lt;dbl&gt; | pH &lt;dbl&gt; | Detect_Concentration &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|---|---|\n",
       "| 1 | 2012-09-24 | 52 | 165 | 41 | 60.0 | 7.7 | 0 |\n",
       "| 2 | 2015-12-17 | 40 | 445 | 42 |  2.0 | 7.3 | 0 |\n",
       "| 3 | 2015-02-02 | 45 | 160 | 40 | 40.0 | 7.4 | 0 |\n",
       "| 4 | 2012-10-22 | 42 | 440 | 57 |  1.5 | 8.0 | 1 |\n",
       "| 5 | 2011-01-03 | 48 | 120 | 42 | 25.0 | 7.1 | 0 |\n",
       "| 6 | 2015-12-15 | 60 | 280 | 32 | 10.0 | 8.2 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "  Water_Sample_Date Casing_Depth Well_Depth Static_Water_Depth Flow_Rate pH \n",
       "1 2012-09-24        52           165        41                 60.0      7.7\n",
       "2 2015-12-17        40           445        42                  2.0      7.3\n",
       "3 2015-02-02        45           160        40                 40.0      7.4\n",
       "4 2012-10-22        42           440        57                  1.5      8.0\n",
       "5 2011-01-03        48           120        42                 25.0      7.1\n",
       "6 2015-12-15        60           280        32                 10.0      8.2\n",
       "  Detect_Concentration\n",
       "1 0                   \n",
       "2 0                   \n",
       "3 0                   \n",
       "4 1                   \n",
       "5 0                   \n",
       "6 1                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arsenic_data = arsenic_data %>%\n",
    "    # Converting `Detect_Concentration from a character to a factor\n",
    "    mutate(Detect_Concentration = relevel(factor(ifelse(Detect_Concentration == \"D\", 1, 0)), ref = \"0\"),\n",
    "        # converting water sample date from a character to a date type \n",
    "        Water_Sample_Date = mdy(Water_Sample_Date)) %>%\n",
    "    # Removing tax id and only keeping the predictor and outcome variables in the dataset\n",
    "    select(-Tax_ID)\n",
    "\n",
    "head(arsenic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be60453a",
   "metadata": {},
   "source": [
    "### Testing for differences in predictor variables acrosss the outcome classes\n",
    "\n",
    "It is useful to run summary statistics on the variables that will be used as predictors in the algorithm to see if there are differences in distributions between the outcomes classes (either non-detect or detect in this case). Typically, greater significance often leads to better predictivity for a certain variable, since the model is better able to separate the classes. We'll use the `tbl_summary()` function from the `gtsummary` package.\n",
    "\n",
    "For more information on the `tbl_summary()` function, check out this helpful [Tutorial](https://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3538a5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>**Characteristic**</th><th scope=col>**N**</th><th scope=col>**0**, N = 515</th><th scope=col>**1**, N = 198</th><th scope=col>**p-value**</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>Water_Sample_Date </td><td>713</td><td>2013-06-05 (979.174260670888)</td><td>2013-03-05 (957.843005291701)</td><td>0.3   </td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>Casing_Depth      </span></td><td>713</td><td><span style=white-space:pre-wrap>74 (33)                      </span></td><td><span style=white-space:pre-wrap>55 (23)                      </span></td><td>&lt;0.001</td></tr>\n",
       "\t<tr><td>Well_Depth        </td><td>713</td><td>301 (144)                    </td><td>334 (128)                    </td><td>0.005 </td></tr>\n",
       "\t<tr><td>Static_Water_Depth</td><td>713</td><td>35 (12)                      </td><td>36 (13)                      </td><td>0.4   </td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>Flow_Rate         </span></td><td>713</td><td><span style=white-space:pre-wrap>25 (33)                      </span></td><td><span style=white-space:pre-wrap>14 (16)                      </span></td><td>&lt;0.001</td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>pH                </span></td><td>713</td><td><span style=white-space:pre-wrap>7.45 (0.55)                  </span></td><td><span style=white-space:pre-wrap>7.82 (0.40)                  </span></td><td>&lt;0.001</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " **Characteristic** & **N** & **0**, N = 515 & **1**, N = 198 & **p-value**\\\\\n",
       " <chr> & <chr> & <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t Water\\_Sample\\_Date  & 713 & 2013-06-05 (979.174260670888) & 2013-03-05 (957.843005291701) & 0.3   \\\\\n",
       "\t Casing\\_Depth       & 713 & 74 (33)                       & 55 (23)                       & <0.001\\\\\n",
       "\t Well\\_Depth         & 713 & 301 (144)                     & 334 (128)                     & 0.005 \\\\\n",
       "\t Static\\_Water\\_Depth & 713 & 35 (12)                       & 36 (13)                       & 0.4   \\\\\n",
       "\t Flow\\_Rate          & 713 & 25 (33)                       & 14 (16)                       & <0.001\\\\\n",
       "\t pH                 & 713 & 7.45 (0.55)                   & 7.82 (0.40)                   & <0.001\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 5\n",
       "\n",
       "| **Characteristic** &lt;chr&gt; | **N** &lt;chr&gt; | **0**, N = 515 &lt;chr&gt; | **1**, N = 198 &lt;chr&gt; | **p-value** &lt;chr&gt; |\n",
       "|---|---|---|---|---|\n",
       "| Water_Sample_Date  | 713 | 2013-06-05 (979.174260670888) | 2013-03-05 (957.843005291701) | 0.3    |\n",
       "| Casing_Depth       | 713 | 74 (33)                       | 55 (23)                       | &lt;0.001 |\n",
       "| Well_Depth         | 713 | 301 (144)                     | 334 (128)                     | 0.005  |\n",
       "| Static_Water_Depth | 713 | 35 (12)                       | 36 (13)                       | 0.4    |\n",
       "| Flow_Rate          | 713 | 25 (33)                       | 14 (16)                       | &lt;0.001 |\n",
       "| pH                 | 713 | 7.45 (0.55)                   | 7.82 (0.40)                   | &lt;0.001 |\n",
       "\n"
      ],
      "text/plain": [
       "  **Characteristic** **N** **0**, N = 515               \n",
       "1 Water_Sample_Date  713   2013-06-05 (979.174260670888)\n",
       "2 Casing_Depth       713   74 (33)                      \n",
       "3 Well_Depth         713   301 (144)                    \n",
       "4 Static_Water_Depth 713   35 (12)                      \n",
       "5 Flow_Rate          713   25 (33)                      \n",
       "6 pH                 713   7.45 (0.55)                  \n",
       "  **1**, N = 198                **p-value**\n",
       "1 2013-03-05 (957.843005291701) 0.3        \n",
       "2 55 (23)                       <0.001     \n",
       "3 334 (128)                     0.005      \n",
       "4 36 (13)                       0.4        \n",
       "5 14 (16)                       <0.001     \n",
       "6 7.82 (0.40)                   <0.001     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arsenic_data %>%\n",
    "    tbl_summary(by = Detect_Concentration,\n",
    "    # Displaying the mean and standard deviation in parantheses for all continuous variables\n",
    "                statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %>%\n",
    "    # Adding a column that displays the total number of samples for each variable\n",
    "    # This will be 713 for all variables since we have no missing data\n",
    "    add_n() %>% \n",
    "    # Adding a column that displays the p value from anova\n",
    "    add_p(test = list(all_continuous() ~ \"aov\")) %>% \n",
    "    # as_flex_table() %>%\n",
    "    # bold(bold = TRUE, part = \"header\")\n",
    "    as_tibble()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38577cf1",
   "metadata": {},
   "source": [
    "All the variables are significantly different between detect and non-detect iAs with the exception of the sample date and the static water depth, therefore the model should predict fairly well.\n",
    "\n",
    "## Setting up Cross Validation\n",
    "\n",
    "As mentioned above, cross validation is done so that the model is trained and tested on different portions of the entire dataset. We'll use 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64cff629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the splits in the dataset are random, a seed is set for reproducibility to ensure the splits are occuring\n",
    "# in the same locations each time the code is run\n",
    "set.seed(12)\n",
    "\n",
    "# 5-fold cross validation\n",
    "# Saving the index (row number) where the 5 splits are occuring\n",
    "# These indices will be iterated through using a loop to create each training and testing datasets\n",
    "arsenic_index = createFolds(arsenic_data$Detect_Concentration, k = 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd76eaf",
   "metadata": {},
   "source": [
    "Within CV, different parameters will be tested, including the number of trees \"grown\" by RF (`ntree_values`) and the number of predictors used in those trees (`mtry_values`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2427958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntree_values = c(50, 250, 500) # number of trees \n",
    "p = dim(arsenic_data)[2] - 1 # number of predictor variables in the dataset\n",
    "mtry_values = c(sqrt(p), p/2, p) # number of predictors to be used in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26883017",
   "metadata": {},
   "source": [
    "## Predicting iAs Detection with a Random Forest (RF) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72462a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 1 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Balanced Accuracy</th><th scope=col>Sensitivity</th><th scope=col>Specificity</th><th scope=col>PPV</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.6200317</td><td>0.3934615</td><td>0.8466019</td><td>0.516777</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 × 4\n",
       "\\begin{tabular}{llll}\n",
       " Balanced Accuracy & Sensitivity & Specificity & PPV\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.6200317 & 0.3934615 & 0.8466019 & 0.516777\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 × 4\n",
       "\n",
       "| Balanced Accuracy &lt;dbl&gt; | Sensitivity &lt;dbl&gt; | Specificity &lt;dbl&gt; | PPV &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| 0.6200317 | 0.3934615 | 0.8466019 | 0.516777 |\n",
       "\n"
      ],
      "text/plain": [
       "  Balanced Accuracy Sensitivity Specificity PPV     \n",
       "1 0.6200317         0.3934615   0.8466019   0.516777"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting the seed again so the predictions are consistent\n",
    "set.seed(12)\n",
    "\n",
    "# Creating an empty dataframe to save the metrics\n",
    "metrics_df = data.frame()\n",
    "\n",
    "# Iterating through the cross validation folds\n",
    "for (i in 1:length(arsenic_index)){\n",
    "    # Training data\n",
    "    data_train = arsenic_data[-arsenic_index[[i]],]\n",
    "    \n",
    "    # Test data\n",
    "    data_test = arsenic_data[arsenic_index[[i]],]\n",
    "    \n",
    "    # Creating empty lists and dataframes to store errors \n",
    "    reg_rf_pred_tune = list()\n",
    "    rf_OOB_errors = list()\n",
    "    rf_error_df = data.frame()\n",
    "    \n",
    "    # Tuning parameters: using ntree and mtry values to determine which combination yields the smallest OOB error \n",
    "    # from the validation datasets\n",
    "    for (j in 1:length(ntree_values)){\n",
    "        for (k in 1:length(mtry_values)){\n",
    "            \n",
    "            # Running RF to tune parameters\n",
    "            reg_rf_pred_tune[[k]] = randomForest(Detect_Concentration ~ ., data = data_train, \n",
    "                                                 ntree = ntree_values[j], mtry = mtry_values[k])\n",
    "            # Obtaining the OOB error\n",
    "            rf_OOB_errors[[k]] = data.frame(\"Tree Number\" = ntree_values[j], \"Variable Number\" = mtry_values[k], \n",
    "                                   \"OOB_errors\" = reg_rf_pred_tune[[k]]$err.rate[ntree_values[j],1])\n",
    "            \n",
    "            # Storing the values in a dataframe\n",
    "            rf_error_df = rbind(rf_error_df, rf_OOB_errors[[k]])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Finding the lowest OOB error using best number of predictors at split\n",
    "    best_oob_errors <- which(rf_error_df$OOB_errors == min(rf_error_df$OOB_errors))\n",
    "\n",
    "    # Now running RF on the entire training set with the tuned parameters\n",
    "    reg_rf <- randomForest(Detect_Concentration ~ ., data = data_train,\n",
    "                               ntree = rf_error_df$Tree.Number[min(best_oob_errors)],\n",
    "                               mtry = rf_error_df$Variable.Number[min(best_oob_errors)])\n",
    "\n",
    "    # Predicting on test set and adding the predicted values as an additional column to the test data\n",
    "    data_test$Pred_Detect_Concentration = predict(reg_rf, newdata = data_test, type = \"response\")\n",
    "\n",
    "    # Obtaining the confusion matrix\n",
    "    matrix = confusionMatrix(data = data_test$Pred_Detect_Concentration, \n",
    "                             reference = data_test$Detect_Concentration, positive = \"1\")\n",
    "    \n",
    "    # Extracting balanced accuracy, sensitivity, specificity, and PPV\n",
    "    matrix_values = data.frame(t(c(matrix$byClass[11])), t(c(matrix$byClass[1:3])))\n",
    "    \n",
    "    # Adding values to df to be averaged across the 5 splits from CV\n",
    "    metrics_df = rbind(metrics_df, matrix_values)\n",
    "}\n",
    "\n",
    "# Taking average\n",
    "metrics_df = metrics_df %>%\n",
    "    summarise(`Balanced Accuracy` = mean(Balanced.Accuracy), Sensitivity = mean(Sensitivity), \n",
    "          Specificity = mean(Specificity), PPV = mean(Pos.Pred.Value))\n",
    "\n",
    "# Viewing the model's performance metrics\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cc5b0",
   "metadata": {},
   "source": [
    "Takeaways from this confusion matrix:\n",
    "\n",
    "+ Overall, the model did a moderate job at predicting if iAs would be detected based on a balanced accuracy of ~0.6\n",
    "+ RF did a poor job of predicting detect data with a sensitivity of 0.4 and a PPV of ~0.5\n",
    "+ The model was significantly better at predicting non-detect data based on a specificity of ~0.8 \n",
    "\n",
    "**I THINK WE COULD DISCUSS THE CLASS IMBALANCE ISSUE AND ADD EITHER SMOTE OR AUC, BUT THE MODEL PERFORMED DECENTLY WELL SO I'M NOT SURE IF THAT'S REALLY NECESSARY.**\n",
    "\n",
    "ALEXIS- I AGREE, I THINK JUST INCORPORATING AUC AS AN ADDITIONAL MODEL PERFORMANCE METRIC IS A GREAT IDEA, AND LET'S LEAVE OUT SMOTE FOR NOW\n",
    "\n",
    "## Additional Resources\n",
    "To learn more check out the following resources: **I WILL LIKELY CHANGE THESE RESOURCES...I THINK WE CAN FIND BETTER ONES** ALEXIS- YES AGREED, LET'S ALSO ASK OTHERS FOR THEIR FAVORITES - KYLE HAS RESPONDED WITH SOME GOOD SUGGESTIONS AT THIS POINT\n",
    "\n",
    "+ [Machine Learning Mastery](https://machinelearningmastery.com/machine-learning-in-r-step-by-step/)\n",
    "+ [Master in Data Science](https://www.mastersindatascience.org/learning/machine-learning-algorithms/decision-tree/)\n",
    "+ [IBM - What is Machine Learning](https://www.ibm.com/topics/machine-learning)\n",
    "+ Machine Learning by Mueller, J. P. (2021). Machine learning for dummies. John Wiley &amp; Sons. \n",
    "## Concluding Remarks\n",
    "\n",
    "In conclusion, this training module has provided an informative introduction to supervised machine learning using classification techniques in R. Machine learning is a powerful tool that can help researchers gain new insights and improve models to analyze complex datasets faster and in a more comprehensive way. The example we've explored demonstrates the utility of supervised machine learning models on datasets with a plethora of features.\n",
    "\n",
    "## Test Your Knowledge \n",
    "\n",
    "1. Using the \"Manganese_Data\", use RF to determine if well water data can be accurate predictors of Manganese detection. The data is structured similarly to the \"Arsenic_Data\" used in this module, however it now includes 4 additional predictor variables:\n",
    "\n",
    "+ `Longtitude`: Longtitude of address (decimal degrees)\n",
    "+ `Latitude`: Latitude of address (decimal degrees)\n",
    "+ `Stream_Distance`: Euclidean distance to the nearest stream (feet)\n",
    "+ `Elevation`: Surface elevation of the sample location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae87403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
