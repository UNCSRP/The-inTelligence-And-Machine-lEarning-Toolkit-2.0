# Unsupervised Machine Learning 

This training module was developed by Dr. David M. Reif with contributions from Lauren E. Koval, Alexis Payton, MS, and Dr. Julia E. Rager

Spring 2024

## Introduction to Machine Learning and Unsupervised Learning
To reiterate what has been discussed in the previous module, machine learning is a field that has great utility in environmental health sciences, often to investigate high-dimensional datasets. The two main classifications of machine learning discussed throughout the TAME Toolkit are supervised and unsupervised machine learning, though additional classifications exist. Previously, we discussed artificial intelligence and supervised machine learning in [TAME 2.0 Module 5.1 Introduction to Machine Learning & Artificial Intelligence](insert link) and [TAME 2.0 Module 5.2 Supervised Machine Learning](insert link).

**Unsupervised machine learning**, as opposed to supervised machine learning, involves training a model on a dataset lacking ground truths or response variables. In this regard, unsupervised approaches are often used to identify underlying patterns in the data. This can provide the analyst with insights into the data that may not otherwise be apparent. Unsupervised machine learning has been used for understanding differences in gene expression patterns of breast cancer patients ([Jezequel et. al, 2015](https://link.springer.com/article/10.1186/s13058-015-0550-y)) and evaluating metabolomic signatures of patients with and without cystic fibrosis ([Laguna et. al, 2015](https://onlinelibrary.wiley.com/doi/full/10.1002/ppul.23225?casa_token=Vqlz3JgGm10AAAAA%3A4UFubAP2r97CKl9PK8oYDfgrcjrs_ZySDzDCx1t3qc6XvQRxOqIwjTn_eQxm_lzX8UQLE0zURJu94fI)).

**Note**: Unsupervised machine learning is used for exploratory purposes and just because it can find associations between data points that doesn't necessarily mean that those associations have merit, are indicative of causal relationships, or have biological implications. 

```{r, echo=FALSE, out.width = "75%", fig.align = 'center', fig.cap = "Figure 2 from Langs G, Röhrich S, Hofmanninger J, Prayer F, Pan J, Herold C, & Prosch H. Machine learning: from radiomics to discovery and routine. *Radiologe*. 2018 Jun 19. doi: 10.1007/s00117-018-0407-3. Figure regenerated here in alignment with its published [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/)."}
knitr::include_graphics("Module5_4_Images/Module5_4_Image1.png")
```


Unsupervised machine learning includes:

+ **Clustering**: Involves grouping elements in a dataset such that the elements in the same group are more similar to each other than to the elements in the other groups.
  + Exclusive (*K*-means)
  + Overlapping
  + Hierarchical 
  + Probabilistic 
+ **Dimensionality reduction**: Focuses on taking high-dimensional data and transforming it into a lower-dimensional space that has fewer features while preserving important information inherent to the original dataset. This is useful, because reducing the number of features makes the data easier to visualize, while trying to maintain the initial integrity of the dataset. 
  + Principal Component Analysis (PCA)
  + Singular Value Decomposition (SVD)

In this module, we'll focus on methods for ***K*-means clustering** and **Principal Component Analysis**. In the following module, [TAME 2.0 Module 5.5 Unsupervised Machine Learning II: Hierachical Clustering](insert link), we'll focus on hierarchical clustering. For further information on types of unsupervised machine learning, check out [Unsupervised Learning](https://cloud.google.com/discover/what-is-unsupervised-learning#section-3). 


<br>

## *K*-Means Clustering
*K*-means is a common clustering algorithm used to partition quantitative data. This algorithm works by first, randomly selecting a pre-specified number of clusters, *k*, across the data space, with each cluster having a data centroid. When using a standard Euclidean distance metric, the distance is calculated from an observation to each centroid, then the observation is assigned to the cluster of the closest centroid. After all observations have been assigned to one of the *k* clusters, the average of all observations in a cluster is calculated, and the centroid for the cluster is moved to the location of the mean. The process then repeats, with the distance computed between the observations and the updated centroids. Observations may be reassigned to the same cluster, or moved to a different cluster if it is closer to another centroid. These iterations continue until there are no longer changes between cluster assignments for observations, resulting in the final cluster assignments that are then carried forward for analysis/interpretation.

Helpful resources on *k*-means clustering include the following: [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf) &
[Towards Data Science](https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a).

<br>

## Principal Component Analysis (PCA)

Principal Component Analysis, or PCA, is a dimensionality-reduction technique used to transform high-dimensional data into a lower dimensional space while trying to preserve as much of the variability in the original data as possible. PCA has strong foundations in linear algebra, so background knowledge of eigenvalues and eigenvectors is extremely useful. Though the mathematics of PCA is beyond the scope of this module, a variety of more in-depth resources on PCA exist including this [Towards Data Science Blog]("https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643"), and this [Sartorius Blog](https://www.sartorius.com/en/knowledge/science-snippets/what-is-principal-component-analysis-pca-and-how-it-is-used-507186#:~:text=Principal%20component%20analysis%2C%20or%20PCA,more%20easily%20visualized%20and%20analyzed.). At a higher level, important concepts in PCA include:

1. PCA partitions variance in a dataset into linearly uncorrelated principal components (PCs), which are weighted combinations of the original features. 

2. Each PC (starting from PC1) summarizes a decreasing percentage of variance.

3. Every instance (e.g. chemical) in the original dataset has a "weight" or score" on each PC.

4. Any combination of PCs can be compared to summarize relationships amongst the instances (e.g. chemicals), but typically it's the first two eigenvectors that capture a majority of the variance.

```{r, echo=FALSE, out.width= "80%", fig.align = 'center'}
knitr::include_graphics("Module5_4_Images/Module5_4_Image2.png")
```

<br>

## Introduction to Training Module
In this activity, we are going to analyze an example dataset of physicochemical property information for chemicals spanning **per- and polyfluoroalkyl substances (PFAS) and statins**. PFAS represent a ubiquitous and pervasive class of man-made industrial chemicals that are commonly used in food packaging, commercial household products such as Teflon, cleaning products, and flame retardants. PFAS are recognized as highly stable compounds that, upon entering the environment, can persist for many years and act as harmful sources of exposure. Statins represent a class of lipid-lowering compounds that are commonly used as pharmaceutical treatments for patients at risk of cardiovascular disease. Because of their common use amongst patients, statins can also end up in water and wastewater effluent, making them of environmental relevance as well.

This training module was designed to evaluate the chemical space of these diverse compounds and to illustrate the utility of unsupervised machine learning methods to differentiate chemical class and make associations between chemical groupings that can inform a variety of environmental and toxicological applications. The two types of machine learning methods that will be employed are *k*-means and PCA (as described in the introduction).


## Training Module's Environmental Health Questions
This training module was specifically developed to answer the following environmental health questions:

1. Can we differentiate between PFAS and statin chemical classes, when considering just the raw physicochemical property variables without applying unsupervised machine learning techniques?
2. If substances are able to be clustered, what are some of the physicochemical properties that seem to be driving chemical clustering patterns derived through *k*-means?
3. How do the data compare when physicochemical properties are reduced using PCA?
4. Upon reducing the data through PCA, which physicochemical property contributes the most towards informing data variance captured in the primary principal component (Comp.1)?
5. If we did not have information telling us which chemical belonged to which class, could we use PCA and *k*-means to accurately predict whether a chemical is a PFAS vs statin?
6. What kinds of applications/endpoints can be better understood and/or predicted, because of these derived chemical groupings?

<br>

## Script Preparations

#### Setting up Environment 

Loading R packages required for this session
```{r, results=FALSE, message=FALSE}
library(ggplot2)
library(pheatmap) #used to make heatmaps. This can be done in ggplot2 but pheatmap is easier and nicer
library(factoextra) # used for PCA
```

Getting help with packages and functions
```{r}
?ggplot2 # Package documentation for ggplot2
?kmeans # Package documentation for kmeans (a part of the standard stats R package, automatically uploaded)
?prcomp # Package documentation for deriving principal components within a PCA (a part of the standard stats R package, automatically uploaded)
?pheatmap # Package documentation for pheatmap
```

Set your working directory
```{r, eval=FALSE, echo=TRUE}
setwd("/filepath to where your input files are") # e.g. setwd("/Downloads")
```

#### Loading the Example Dataset
Let's start by loading the datasets needed for this training module. We are going to use a dataset of substances that have a diverse chemical space of PFAS and statin compounds. This list of chemicals will be uploaded alongside physicochemical property data. The chemical lists for 'PFAS' and 'Statins' were obtained from the EPA's Computational Toxicology Dashboard [Chemical Lists](https://comptox.epa.gov/dashboard/chemical-lists). The physicochemical properties were obtained by uploading these lists into the National Toxicology Program’s [Integrated Chemical Environment (ICE)](https://ice.ntp.niehs.nih.gov/). 
```{r}
dat <- read.csv("Module5_4_InputData.csv", fileEncoding = "UTF-8-BOM")
```

#### Data Viewing

Starting with the overall dimensions:
```{r}
dim(dat)
```

Then looking at the first four rows and five columns of data:
```{r}
dat[1:4,1:5]
```

Note that the first column, `List`, designates the following two larger chemical classes:
```{r}
unique(dat$List)
```

Let's lastly view all of the column headers:
```{r}
colnames(dat)
```

In the data file, the first four columns represent chemical identifier information. All remaining columns represent different physicochemical properties derived from OPERA via [Integrated Chemical Environment (ICE)](https://ice.ntp.niehs.nih.gov/). Because the original titles of these physicochemical properties contained commas and spaces, R automatically converted these into periods. Hence, titles like `OPERA..Boiling.Point`.

For ease of downstream data analyses, let's create a more focused dataframe option containing only one chemical identifier (CASRN) as row names, and then just the physicochemical property columns.
```{r}
# Creating a new dataframe that contains the physiocochemical properties
chemical_prop_df <- dat[,5:ncol(dat)]
rownames(chemical_prop_df) <- dat$CASRN
```

Now explore this data subset:
```{r}
dim(chemical_prop_df) # overall dimensions
chemical_prop_df[1:4,1:5] # viewing the first four rows and five columns
colnames(chemical_prop_df)
```
<br>

### Evaluating the Original Physicochemical Properties across Substances

Let's first plot two physicochemical properties to determine if and how substances group together without any fancy data reduction or other machine learning techniques. This will answer **Environmental Health Question #1**: Can we differentiate between PFAS and statin chemical classes, when considering just the raw physicochemical property variables without applying unsupervised machine learning techniques?

Let's put molecular weight (`Molecular.Weight`) as one axis and boiling point (`OPERA..Boiling.Point`) on the other. We'll also color by the chemical classes using the `List` column from the original dataframe.
```{r}
ggplot(chemical_prop_df[,1:2], aes(x = Molecular.Weight, y = OPERA..Boiling.Point, color = dat$List)) + 
  geom_point(size = 2) + theme_bw() + 
  ggtitle('Version A: Bivariate Plot of Two Original Physchem Variables') + 
  xlab("Molecular Weight") + ylab("Boiling Point")
```

Let's plot two other physicochemical property variables, Henry's Law constant (`OPERA..Henry.s.Law.Constant`) and melting point (`OPERA..Melting.Point`), to see if the same separation of chemical classes is apparent. 
```{r}
ggplot(chemical_prop_df[,3:4], aes(x = OPERA..Henry.s.Law.Constant, y = OPERA..Melting.Point, 
                                                  color = dat$List)) + 
  geom_point(size = 2) + theme_bw() + 
  ggtitle('Version B: Bivariate Plot of Two Other Original Physchem Variables') + 
  xlab("OPERA..Henry.s.Law.Constant") + ylab("OPERA..Melting.Point")
```

### Answer to Environmental Health Question 1
*With these, we can answer **Environmental Health Question #1***: Can we differentiate between PFAS and statin chemical classes, when considering just the raw physicochemical property variables without applying machine learning techniques?

**Answer**: Only in part. From the first plot, we can see that PFAS tend to have lower molecular weight ranges in comparison to the statins, though other property variables clearly overlap in ranges of values, making the groupings not entirely clear.

<br>

## Identifying Clusters of Chemicals through *K*-Means

Let's turn out attention to **Environmental Health Question #2**: If substances are able to be clustered, what are some of the physicochemical properties that seem to be driving chemical clustering patterns derived through *k*-means? This will be done deriving clusters of chemicals, based on ALL underlying physicochemical property data, using *k*-means clustering.

For this example, let's coerce the *k*-means algorithms to calculate 2 distinct clusters (based on their corresponding mean centered values). Here we choose to derive two distinct clusters, because we are ultimately going to see if we can use this information to predict each chemical's classification into two distinct chemical classes (i.e., PFAS vs statins). Note that we can derive more clusters using similar code, depending on the question being addressed.

We can give a name to this variable, to easily provide the number of clusters in the next lines of code, `num.centers`:
```{r}
num.centers <- 2
```

Here we derive chemical clusters using *k*-means:
```{r}
clusters <- kmeans(chemical_prop_df,                  # input dataframe
                   centers = num.centers,  # number of cluster centers to calculate
                   iter.max = 1000,        # the maximum number of iterations allowed
                   nstart = 50)            # the number of rows used as the random set for the initial centers (during the first iteration)
```

The resulting property values that were derived as the final cluster centers can be pulled using:
```{r}
clusters$centers
```

Let's add the cluster assignments to the physicochemical data and create a new dataframe, which can then be used in a heatmap visualization to see how these physicochemical data distributions clustered according to *k*-means.

These cluster assignments can be pulled from the `cluster` list output, where chemicals are designated to each cluster with either a 1 or 2. You can view these using:
```{r}
clusters$cluster
```

Because these results are listed in the exact same order as the inputted dataframe, we can simply add these assignments to the `chemical_prop_df` dataframe.
```{r}
dat_wclusters <- cbind(chemical_prop_df,clusters$cluster)
colnames(dat_wclusters)[11] <- "kmeans_cluster"  # renaming this new column "kmeans_cluster"
dat_wclusters <- dat_wclusters[order(dat_wclusters$kmeans_cluster),]  # sorting data by cluster assignments
```
<br>

To generate a heatmap, we need to first create a separate dataframe for the cluster assignments, ordered in the same way as the physicochemical data:
```{r}
hm_cluster <- data.frame(dat_wclusters$kmeans_cluster, row.names = row.names(dat_wclusters))  # creating the dataframe
colnames(hm_cluster) <- "kmeans_cluster"   # reassigning the column name
hm_cluster$kmeans_cluster <- as.factor(hm_cluster$kmeans_cluster)   # coercing the cluster numbers into factor variables, to make the heat map prettier

head(hm_cluster)  # viewing this new cluster assignment dataframe
```

Then we can call this dataframe, as well as the main physicochemical property dataframe (both sorted by clusters) into the following heatmap visualization code, leveraging the `pheatmap()` function. This function was designed specifically to enable clustered heatmap visualizations. Check out [pheatmap Documenation](https://www.rdocumentation.org/packages/pheatmap/versions/1.0.12/topics/pheatmap) for additional information. 

### Heatmap Visualization of the Resulting *K*-Means Clusters
```{r, fig.height=8, fig.width=10}
chem_hm <- pheatmap(dat_wclusters[,1:10], 
            main = "Heatmap of Physicochemical Properties with k-means Cluster Assignments",
            cluster_rows = FALSE, cluster_cols = FALSE, # no further clustering, for simplicity
            scale = "column",    # scaling the data to make differences across chemicals more apparent
            annotation_row = hm_cluster, # calling the cluster assignment dataframe as a separate color bar
            angle_col = 45, fontsize_col = 7, fontsize_row = 3, # adjusting size/ orientation of axes labels
            cellheight = 3, cellwidth = 25, # setting height and width for cells
            border_color = FALSE # specify no border surrounding the cells
)
```
Shown here is a heat map displaying the relative values for each physicochemical property, with all 10 properties listed along the bottom. Individual chemicals are listed along the right hand side. The *k*-means cluster assignment is provided as a separate color bar on the left.

### Answer to Environmental Health Question 2
*With this, we can answer **Environmental Health Question #2***: What are some of the physicochemical properties that seem to be driving chemical clustering patterns derived through *k*-means?

**Answer**: Properties with values that show obvious differences between resulting clusters including molecular weight, boiling point, negative log of acid dissociation constant, octanol air partition coefficient, and octanol water distribution coefficient.

<br>

## Principal Component Analysis (PCA)
Next, we will run through some example analyses applying the common data reduction technique of PCA. We'll start by determining how much of the variance is able to be captured within the first two principal components to answer **Environmental Health Question #3**: How do the data compare when physicochemical properties are reduced using PCA? 


We can calculate the principal components across ALL physicochemical data across all chemicals using the `prcomp()` function. Always make sure your data is centered and scaled prior to running to PCA, since it's sensitive to variables having different scales. 
```{r}
my.pca <- prcomp(chemical_prop_df,   # input dataframe of physchem data
                   scale = TRUE, center = TRUE) 
```

We can see how much of the variance was able to be captured in each of the eigenvectors or dimensions using a scree plot. 
```{r}
fviz_eig(my.pca, addlabels = TRUE)
```

We can also calculate these values, and pull them into a dataframe for future use. For example, to pull the percentage of variance explained by each principal component, we can run the following calculations, where first eigenvalues (eigs) are calculated and then used to calculate percent of variance, per principal component:
```{r}
eigs <- my.pca$sdev^2
Comp.stats <- data.frame(eigs, eigs/sum(eigs), row.names = names(eigs))
colnames(Comp.stats) <- c("Eigen_Values", "Percent_of_Variance")

head(Comp.stats)
```
### Answer to Environmental Health Question 3
*With this, we can answer **Environmental Health Question #3***: How do the data compare when physicochemical properties are reduced using PCA?

**Answer**: Principal Component 1 captures ~41% of the variance and Principal Component 2 captures ~24% across all physicochemical property values, across all chemicals. These two components together describe ~65% of data.

<br>

Next, we'll use PCA to answer **Environmental Health Question #4**: Upon reducing the data through PCA,which physicochemical property contributes the most towards informing data variance captured in the primary principal component (Comp.1)?

Here are the resulting scores for each chemical's contribution towards each principal component (shown here as components `PC1`-`PC10`).
```{r}
head(my.pca$x)
```

And the resulting loading factors of each property's contribution towards each principal component. 
```{r}
my.pca$rotation
```
### Answer to Environmental Health Question 4
*With these results, we can answer **Environmental Health Question #4***: Upon reducing the data through PCA, which physicochemical property contributes the most towards informing data variance captured in the primary principal component (Comp.1)?

**Answer**: Boiling point contributes the most towards principal component 1, as it has the largest magnitude (0.464).

<br>


#### Visualizing PCA Results

Let's turn our attention to **Environmental Health Question #5**: If we did not have information telling us which chemical belonged to which class, could we use PCA and *k*-means to accurately predict whether a chemical is a PFAS vs statin?

We can start by answering this question be visualizing the first two principal components and coloring each chemical according to class (i.e. PFAS vs statins).
```{r}
ggplot(data.frame(my.pca$x), aes(x = PC1, y = PC2, color = dat$List)) + 
  geom_point(size = 2) + theme_bw() + 
  ggtitle('Version C: PCA Plot of the First 2 PCs, colored by Chemical Class') + 
  xlab("Principal Component 1 (40.9%") + ylab("Principal Component 2 (23.8%)")
```

### Answer to Environmental Health Question 5
*With this, we can answer **Environmental Health Question #5***: If we did not have information telling us which chemical belonged to which class, could we use PCA and *k*-means to accurately predict whether a chemical is a PFAS vs statin?

 **Answer**: Data become more compressed and variables reduce across principal components capturing the majority of the variance (~65%). This results in improved data visualizations, where all dimensions of the physiochemical dataset are compressed and captured across the displayed components. In addition, the figure above shows a clear separation between PFAS and statin chemical when visualizing the reduced dataset. 

<br>

## Incorporating *K*-Means into PCA for Predictive Modeling

We can also identify cluster-based trends within data that are reduced after running PCA. This example analysis does so, expanding upon the previously generated PCA results.

### Estimate *K*-Means Clusters from PCA Results

Let's first run code, similar to the previous *k*-means analysis and associated parameters, though instead here we will use data reduced values from the PCA analysis. Specifically, clusters across PCA "scores" values will be derived, where scores represent the relative amount each chemical contributed to each principal component.
```{r}
clusters_PCA <- kmeans(my.pca$x, centers = num.centers, iter.max = 1000, nstart = 50)
```

The resulting PCA score values that were derived as the final cluster centers can be pulled using:
```{r}
clusters_PCA$centers
```

Viewing the final cluster assignment, per chemical:
```{r}
head(cbind(rownames(chemical_prop_df),clusters_PCA$cluster))
```  

<br>

#### Visualizing *K*-Means Clusters from PCA Results

Let's now view, again, the results of the main PCA, focusing on the first two principal components; though this time let's color each chemical according to *k*-means cluster.
```{r}
ggplot(data.frame(my.pca$x), aes(x = PC1, y = PC2, color = as.factor(clusters_PCA$cluster))) + 
  geom_point(size = 2) + theme_bw() + 
  ggtitle('Version D: PCA Plot of the First 2 PCs, colored by k-means Clustering') + 
  # it's good practice to put the percentage of the variance capture in the axes titles
  xlab("Principal Component 1 (40.9%") + ylab("Principal Component 2 (23.8%)")
```

### Answer to Environmental Health Question 6
*With this we can answer **Environmental Health Question #6***: What kinds of applications/endpoints can be better understood and/or predicted, because of these derived chemical groupings?

**Answer**: With these well-informed chemical groupings, we can now better understand the variables that attribute to the chemical classifications. We can also use this information to better understand data trends and predict environmental fate and transport for these chemicals. The reduced variables derived through PCA, and/or *k*-means clustering patterns can also be used as input variables to predict toxicological outcomes.

<br>

## Concluding Remarks
In conclusion, this training module provide an example exercise on organizing physicochemical data and analyzing trends within these data to determine chemical groupings. Results are compared from those produced using just the original data vs. clustered data from *k*-means vs. reduced data from PCA. These methods represent common tools that are used in high dimensional data analyses within the field of environmental health sciences.

<br>

### Test Your Knowledge 

In this training module, we presented an unsupervised machine learning example that was based on defining *k*-means clusters based on chemical class where *k* = 2. Often times, analyses are conducted to explore potential clustering relationships without an preexisting idea of what *k* or the number of clusters should be. In this test your knowledge section, we'll go through an example like that. 

Using the accompanying flame retardant and pesticide physicochemical property variables found in the file ("Module5_4_TYKInput.csv"), answer the following questions:

1. What are some of the physicochemical properties that seem to be driving chemical clustering patterns derived through *k*-means?
2. Upon reducing the data through PCA, which physicochemical property contributes the most towards informing data variance captured in the primary principal component (Comp.1)?
3. Construct a PCA plot of the first two PCs for this dataset and color the points by `List`. 
