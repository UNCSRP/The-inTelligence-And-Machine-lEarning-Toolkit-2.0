# Supervised Machine Learning - Test Your Knowledge Solution

```{r warning=FALSE, results='hide', message=FALSE}
library(readxl);
library(lubridate);
library(tidyverse);
library(gtsummary);
library(flextable);
library(caret);
library(randomForest);
```


```{r}
# Load the data
manganese_data <- data.frame(read_excel("Module5_2TYKInput.xlsx"))

# View the top of the dataset
head(manganese_data) 
```

Like we did in the module, we'll start by changing some of the data types. 
```{r}
manganese_data = manganese_data %>%
    # Converting `Detect_Concentration from a character to a factor
    mutate(Detect_Concentration = relevel(factor(Detect_Concentration), ref = "ND"),
        # converting water sample date from a character to a date type 
        Water_Sample_Date = mdy(Water_Sample_Date)) %>%
    # Removing tax id and only keeping the predictor and outcome variables in the dataset
    select(-Tax_ID)

head(manganese_data)
```


Testing for differences in predictor variables across the outcome classes.
```{r}
manganese_data %>%
    tbl_summary(by = Detect_Concentration,
    # Selecting columns to include
    include = colnames(manganese_data[c(2:11)]), 
    # Displaying the mean and standard deviation in parantheses for all continuous variables
                statistic = list(all_continuous() ~ "{mean} ({sd})")) %>%
    # Adding a column that displays the total number of samples for each variable
    # This will be 713 for all variables since we have no missing data
    add_n() %>% 
    # Adding a column that displays the p value from anova
    add_p(test = list(all_continuous() ~ "aov")) %>% 
    as_flex_table() %>%
    bold(bold = TRUE, part = "header")
```

4 predictor variables (casing depth, pH, longtitude, and latitude) are significantly different, so the model should be able to predict moderately well. 

Next, setting up cross validation and parameters to be tuned.
```{r}
set.seed(12)

# Establish a list of indices that will used to identify our training and testing data with a 60-40 split
tt_indices <- createDataPartition(y=manganese_data$Detect_Concentration, p = 0.6, list=FALSE)

# Use indices to make our training and testing datasets and view the number of Ds (1) and NDs (0) 
Mn_train <- manganese_data[tt_indices,]
table(Mn_train$Detect_Concentration)

Mn_test <- manganese_data[-tt_indices,]
table(Mn_test$Detect_Concentration)

```

Predicting with RF
```{r}
# Setting the seed again so the predictions are consistent
set.seed(12)

control <- trainControl(method = 'cv',
                        number = 5,
                        search = 'grid')

# Establish grid of predictors to test in our model as part of hyper parameter tuning
p <- ncol(manganese_data)-1 # p is the total number of predictors in the dataset
tunegrid_rf <- expand.grid(mtry = c(floor(sqrt(p)), p/2, p)) # We will test sqrt(p), p/2, and p predictors

# Look at the column names in training dataset
colnames(Mn_train)

rf_train <- train(x=Mn_train[,1:p], # Specify predictor variables
                         y = Mn_train[,p+1], # Specify outcome variables
                         trControl = control, # Specify the cross-validation parameters we defined above
                         method = 'rf', # Specify we want to train a Random Forest
                         importance = TRUE, # This parameter calculates the variable importance for RF models specifically which can help with downstream analyses
                         tuneGrid = tunegrid_rf, # Specify the number of predictors we want to test as defined above
                         metric = "Accuracy") # Specify what evaluation metric we want to use to decide which model is the best

# Look at the results of training
rf_train

# Save the best model from our training
rf_final <- rf_train$finalModel

# View confusion matrix for best model
rf_final


# Use our best model to predict the classes for our test data. We need to make sure we remove the column of Ds/NDs from our test data.
rf_res <- predict(rf_final, Mn_test %>% 
                      select(!Detect_Concentration))

# View a confusion matrix of the results and gauge model performance
confusionMatrix(rf_res, Mn_test$Detect_Concentration, positive = "D")
```

**Answer**: Here are some takeaways from this confusion matrix..

+ Overall, the model performed moderately well at predicting if iAs would be detected based on an overall accuracy of ~0.7
+ RF did an okay job of predicting detect data with a sensitivity of ~0