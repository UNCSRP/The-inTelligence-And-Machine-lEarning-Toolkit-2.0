{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b0bcea4-ddcf-4052-b26c-cc7cc751ca7e",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Model Interpretation\n",
    "\n",
    "This training module was developed by Alexis Payton, MS, Lauren Koval, and Dr. Julia E. Rager\n",
    "\n",
    "Fall 2023\n",
    "\n",
    "## Introduction to Supervised Machine Learning Model Interpretation\n",
    "Supervised machine learning can utilize a multitude of predictors unlike traditional statistical analyses, which allows these models to more closely resemble real-world, complex environmental health scenarios offering new insights. However, one disadvantage of machine learning is that it is not as easily interpretable as traditional statistics (ie. linear regression). That being said, there are methods and concepts that can be applied to supervised machine learning algorithms to aid in the understanding of its predictions including:\n",
    "\n",
    "+ Variable (Feature) Importance\n",
    "+ Decision Boundaries\n",
    "  \n",
    "Visualization of these methods is an important part of interpretability, since visualizing helps convey concepts faster.\n",
    "\n",
    "## Variable Importance\n",
    "\n",
    "When a supervised machine learning (ML) algorithm makes predictions, it relies more heavily on some variables than others. How much a variable contributes to classifying data is known as **variable (feature) importance**. There are many methods that are used to measure feature importance, however in this module we'll be highlighting mean decrease gini that can easily be extracted from random forest (RF) classification-based models. Mean decrease gini (gini impurity) simply quantifies the improvement of predictivity with the addition of each predictor in a decision tree, which is then averaged over all the decision trees tested. The higher the value the greater the importance on the algorithm. \n",
    "\n",
    "Note for RF regression-based models, node purity can be extracted as a measure of feature importance. For more information regarding feature importance and mean decrease gini see:\n",
    "+ [Feature Importance](https://www.baeldung.com/cs/ml-feature-importance)\n",
    "+ [Mean Decrease Gini](https://cran.r-project.org/web/packages/rfVarImpOOB/vignettes/rfVarImpOOB-vignette.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe4ab6-a26f-4f89-b730-8de107c41daa",
   "metadata": {},
   "source": [
    "## Decision Boundary \n",
    "Another concept that is pertinent to a model's interpretability, is understanding a decision boundary and how visualizing it can further aid in understanding how the model classifies new data points. A **decision boundary** is a line (or a hyperplane) that seeks to separate the training data by class. This line can be linear or non-linear and is formed in n-dimensional space. To clarify, although support vector machine (SVM) specifically uses decision boundaries to classify training data and make predictions on test data, decision boundaries can still be drawn for other algorithms.  \n",
    "\n",
    "A decision boundary can be visualized to convey to the reader in a more succinct manner how well an algorithm is able to classify an outcome based on the data given. It is important to note that most ML models make use of datasets that contain 3 or more predictors and it is difficult to visualize a plot in more than three dimensions. Therefore, the number of features and which features to plot need to be narrowed down to two variables. For this reason, the resulting visualization is not a true representation of the decision boundary from the initial model, since the visualization only relies on prediction results from two predictors. Nevertheless, decision boundary plots can be powerful figures. \n",
    "\n",
    "When choosing variables for decision boundary plots, features that have the most influence on the model are often selected, but that is not always the case. Sometimes predictors are selected based upon the environmental health implications relevant to the research question. For example in [Perryman et. al](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285721), lung response resulting from ozone exposure was investigated by sampling derivatives of cholesterol biosynthesis in human subjects. In this paper, these sterol metabolites were used to predict whether a subject would be classified as having a lung response that was considered to be non-responsive or responsive. A decision boundary plot was made using two predictors:\n",
    "\n",
    "+ Cholesterol, given that it had the highest variable importance and\n",
    "+ Vitamin D, given its synthesis can be affected by ozone despite it having a lower variable importance in the paper's models.\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/c69e7341-97bc-4d1b-a718-7a0f979c9830\" width=\"684\" />\n",
    "\n",
    "Takeaways from this decision boundary plot:\n",
    "+ Subjects with more lung inflammation (\"responders\") after ozone exposure tended to have higher Vitamin D levels and lower Cholesterol levels.\n",
    "+ Thse \"responder\" subjects were more likely to be non-asthmatics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab7b45-0895-4124-bf0d-b6e2fac0929c",
   "metadata": {},
   "source": [
    "## Introduction to Training Module\n",
    "In the previous module, we investigated whether a classification-based RF model using well water variables would be accurate predictors of iAs detection. In this module, we will extract variable importance from the same algorithm and plot variable importance. The two features with the highest variable importance will be identified and used to construct a decision boundary plot to determine how features are associated with iAs detection. \n",
    "\n",
    "The data to be used in this module was described and referenced previously in [TAME Toolkit module 5](insert link here).\n",
    "\n",
    "## Training Module's Environmental Health Questions\n",
    "\n",
    "This training module was specifically developed to answer the following environmental health questions:\n",
    "\n",
    "1. What is each variable's importance on the predictive accuracy of iAs detection from a RF algorithm?\n",
    "\n",
    "2. How can variable importance of the features used in iAs detection be plotted from highest to lowest importance?\n",
    "   \n",
    "3. Using the two features with the highest variable importance, how can a decision boundary be visualized to separate and predict iAs detection?\n",
    "\n",
    "## Script Preparations\n",
    "\n",
    "### Cleaning the global environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221ba17a-2541-4416-8dc3-1e1d9296b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list=ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb178c4-645f-477e-9e4a-de553b1d528b",
   "metadata": {},
   "source": [
    "### Installing required R packages\n",
    "If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4a648f-b7a6-4f7e-986f-62ec884ca06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required namespace: readxl\n",
      "\n",
      "Loading required namespace: lubridate\n",
      "\n",
      "Loading required namespace: tidyverse\n",
      "\n",
      "Loading required namespace: caret\n",
      "\n",
      "Loading required namespace: randomForest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if (!requireNamespace(\"readxl\"))\n",
    "  install.packages(\"readxl\");\n",
    "if (!requireNamespace(\"lubridate\"))\n",
    "  install.packages(\"lubridate\");\n",
    "if (!requireNamespace(\"tidyverse\"))\n",
    "  install.packages(\"tidyverse\");\n",
    "if (!requireNamespace(\"caret\"))\n",
    "  install.packages(\"caret\");\n",
    "if (!requireNamespace(\"randomForest\"))\n",
    "  install.packages(\"randomForest\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050ac15-9546-4970-9074-8b422af25726",
   "metadata": {},
   "source": [
    "### Loading R packages required for this session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28eb3b06-5108-49a3-a4f6-49d7888791be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘lubridate’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    date, intersect, setdiff, union\n",
      "\n",
      "\n",
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.1.3     \u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.5.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.4.3     \u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 1.0.2     \u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.3.0\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "Loading required package: lattice\n",
      "\n",
      "\n",
      "Attaching package: ‘caret’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    lift\n",
      "\n",
      "\n",
      "randomForest 4.7-1.1\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(readxl);\n",
    "library(lubridate);\n",
    "library(tidyverse);\n",
    "library(caret);\n",
    "library(randomForest);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6ee900-cfe7-4f14-848a-238c6fde33f8",
   "metadata": {},
   "source": [
    "### Set your working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21851486-617a-42a0-8444-6575044d2ae9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in setwd(\"/filepath to where your input files are\"): cannot change working directory\n",
     "output_type": "error",
     "traceback": [
      "Error in setwd(\"/filepath to where your input files are\"): cannot change working directory\nTraceback:\n",
      "1. setwd(\"/filepath to where your input files are\")"
     ]
    }
   ],
   "source": [
    "setwd(\"/filepath to where your input files are\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb70feea-7cfb-4d32-a116-273677871c35",
   "metadata": {},
   "source": [
    "### Importing example dataset\n",
    "**Will need to change input to module number and add module number to the file itself**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85ad6d-7cfc-431e-b791-c12581f49b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "arsenic_data <- data.frame(read_excel(\"Module5/Module5_Arsenic_Data.xlsx\"))\n",
    "\n",
    "# View the top of the dataset\n",
    "head(arsenic_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855e0d8-560b-4288-9b6e-bf7c528ee2be",
   "metadata": {},
   "source": [
    "### Changing Data Types \n",
    "First, `Detect_Concentration` needs to be converted from a character to a factor, so that Random Forest to know that non-detect data is considered baseline. `Water_Sample_Date` needs to be converted from a character to a date type, so that Random Forest understands this column contains dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad5844-c3b3-4900-ae06-c29cec47256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arsenic_data = arsenic_data %>%\n",
    "    # Converting `Detect_Concentration` from a character to a factor\n",
    "    mutate(Detect_Concentration = relevel(factor(Detect_Concentration ), ref = \"ND\"),\n",
    "        # Converting water sample date from a character to a date type \n",
    "        Water_Sample_Date = mdy(Water_Sample_Date)) %>%\n",
    "        # Removing tax id and only keeping the predictor and outcome variables in the dataset\n",
    "        # This allows us to put the entire dataframe as is into RF\n",
    "        select(-Tax_ID)\n",
    "\n",
    "head(arsenic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4a991-01cb-4a77-8102-1c75ad841221",
   "metadata": {},
   "source": [
    "## Setting up Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af98073-c4d4-402c-9d7f-822644bd2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed for reproducibility\n",
    "set.seed(12)\n",
    "\n",
    "# 5-fold cross validation\n",
    "arsenic_index = createFolds(arsenic_data$Detect_Concentration, k = 5) \n",
    "\n",
    "# Creating vectors for parameters to be tuned\n",
    "ntree_values = c(50, 250, 500) # number of decision trees \n",
    "p = dim(arsenic_data)[2] - 1 # number of predictor variables in the dataset\n",
    "mtry_values = c(sqrt(p), p/2, p) # number of predictors to be used in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f13eb-c4e2-4d8f-b611-e9d105dbb72f",
   "metadata": {},
   "source": [
    "## Predicting iAs Detection with a Random Forest (RF) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404984b-1981-457a-a5dd-d413d855be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed again so the predictions are consistent\n",
    "set.seed(12)\n",
    "\n",
    "# Creating an empty dataframe to save the variable importance\n",
    "variable_importance_df = data.frame()\n",
    "\n",
    "# Iterating through the cross validation folds\n",
    "for (i in 1:length(arsenic_index)){\n",
    "    # Training data\n",
    "    data_train = arsenic_data[-arsenic_index[[i]],]\n",
    "    \n",
    "    # Test data\n",
    "    data_test = arsenic_data[arsenic_index[[i]],]\n",
    "    \n",
    "    # Creating empty lists and dataframes to store errors \n",
    "    reg_rf_pred_tune = list()\n",
    "    rf_OOB_errors = list()\n",
    "    rf_error_df = data.frame()\n",
    "    \n",
    "    # Tuning parameters: using ntree and mtry values to determine which combination yields the smallest OOB error \n",
    "    # from the validation datasets\n",
    "    for (j in 1:length(ntree_values)){\n",
    "        for (k in 1:length(mtry_values)){\n",
    "            \n",
    "            # Running RF to tune parameters\n",
    "            reg_rf_pred_tune[[k]] = randomForest(Detect_Concentration ~ ., data = data_train, \n",
    "                                                 ntree = ntree_values[j], mtry = mtry_values[k])\n",
    "            # Obtaining the OOB error\n",
    "            rf_OOB_errors[[k]] = data.frame(\"Tree Number\" = ntree_values[j], \"Variable Number\" = mtry_values[k], \n",
    "                                   \"OOB_errors\" = reg_rf_pred_tune[[k]]$err.rate[ntree_values[j],1])\n",
    "            \n",
    "            # Storing the values in a dataframe\n",
    "            rf_error_df = rbind(rf_error_df, rf_OOB_errors[[k]])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Finding the lowest OOB error using best number of predictors at split\n",
    "    best_oob_errors <- which(rf_error_df$OOB_errors == min(rf_error_df$OOB_errors))\n",
    "\n",
    "    # Now running RF on the entire training set with the tuned parameters\n",
    "    reg_rf <- randomForest(Detect_Concentration ~ ., data = data_train,\n",
    "                               ntree = rf_error_df$Tree.Number[min(best_oob_errors)],\n",
    "                               mtry = rf_error_df$Variable.Number[min(best_oob_errors)])\n",
    "\n",
    "    # Predicting on test set and adding the predicted values as an additional column to the test data\n",
    "    data_test$Pred_Detect_Concentration = predict(reg_rf, newdata = data_test, type = \"response\")\n",
    "\n",
    "    # extracting variable importance\n",
    "    variable_importance_values = data.frame(importance(reg_rf)) %>%\n",
    "        rownames_to_column(var = \"Predictor\")\n",
    "    variable_importance_df = rbind(variable_importance_df, variable_importance_values)\n",
    "}\n",
    "\n",
    "# Taking average\n",
    "variable_importance_df = variable_importance_df %>%\n",
    "    group_by(Predictor) %>%\n",
    "    summarise(MeanDecreaseGini = mean(MeanDecreaseGini)) %>%\n",
    "    # Sorting from highest to lowest\n",
    "    arrange(-MeanDecreaseGini)\n",
    "\n",
    "# Viewing the model's variable importance\n",
    "variable_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ef727-0400-44c2-b04c-adafaa54747b",
   "metadata": {},
   "source": [
    "### Reformatting the dataframe for plotting \n",
    "Transforming the dataframe so that figure is more legible. Specifically, spaces will be added between the variables and the `Predictor` column will be put into a factor to rearrange the order of the variables from lowest to highest mean decrease gini. For additional information on improving visualization see [TAME Toolkit Module 3.4](INSERT LINK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472905b5-73bb-4a22-876d-99aa2f8c961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding spaces between the variables that need the space\n",
    "modified_variable_importance_df = variable_importance_df %>%\n",
    "     mutate(Predictor = ifelse(Predictor == \"Casing_Depth\", \"Casing Depth\",\n",
    "                ifelse(Predictor == \"Water_Sample_Date\", \"Water Sample Date\",\n",
    "                    ifelse(Predictor == \"Flow_Rate\", \"Flow Rate\",\n",
    "                        ifelse(Predictor == \"Well_Depth\", \"Well Depth\",\n",
    "                            ifelse(Predictor == \"Static_Water_Depth\", \"Static Water Depth\",  \n",
    "                                          Predictor)))))))\n",
    "\n",
    "# Saving the order of the variables from lowest to highest mean decrease gini by putting into a factor\n",
    "predictor_order = rev(modified_variable_importance_df$Predictor)\n",
    "modified_variable_importance_df$Predictor = factor(modified_variable_importance_df$Predictor, levels = predictor_order)\n",
    "\n",
    "head(modified_variable_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d4ca2-9e0c-4a47-8db3-ed91b82ee87c",
   "metadata": {},
   "source": [
    "## Variable Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408a35c-b1be-4ff0-bb2f-0f98b1ad660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=5) #changing plot size\n",
    "\n",
    "ggplot(data = modified_variable_importance_df , \n",
    "             aes(x = MeanDecreaseGini, y = Predictor, size = 3)) + \n",
    "  geom_point() + \n",
    "\n",
    "  theme_light() + \n",
    "  theme(axis.line = element_line(color = \"black\"), #making x and y axes black\n",
    "        axis.text = element_text(size = 12), #changing size of x axis labels\n",
    "        axis.title = element_text(face = \"bold\", size = rel(1.7)), #changes axis titles\n",
    "        legend.title = element_text(face = 'bold', size = 14), #changes legend title\n",
    "        legend.text = element_text(size = 12), #changes legend text\n",
    "        strip.text.x = element_text(size = 15, face = \"bold\"), #changes size of facet x axis \n",
    "        strip.text.y = element_text(size = 15, face = \"bold\")) + #changes size of facet y axis \n",
    "  labs(x = 'Variable Importance', y = 'Predictor') + #changing axis labels \n",
    "  \n",
    "  guides(size = \"none\")#removing size legend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b270e-ed2e-4f41-bcae-e941a2bbc63e",
   "metadata": {},
   "source": [
    "From the variable importance dataframe and plot, we can see that casing depth had the greatest impact on RF followed by pH, water sample date, flow rate, well depth, and static water depth in descending order. \n",
    "\n",
    "Since casing depth and pH have been identified as the predictors with the highest variable importance, they will be used as the two predictors for the decision boundary plot.\n",
    "\n",
    "### Decision Boundary Calculation\n",
    "First, RF will be trained with casing depth and pH. Since, the decision boundary plot will be used for visualization purposes and a 2-D figure can only plot two variables, we will not worry about tuning the parameters as was previously done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f887587e-5d4b-4011-b4a5-2d6a9c46907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with variables based on the highest predictors\n",
    "highest_predictivity_data = data.frame(arsenic_data[,c(\"Casing_Depth\", \"pH\", \"Detect_Concentration\")])\n",
    "\n",
    "# Training RF\n",
    "rf_detect_arsenic = randomForest(Detect_Concentration~., data = highest_predictivity_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a95128-d2cd-48b0-ad19-746d6720d59b",
   "metadata": {},
   "source": [
    "From this RF prediction, the decision boundary will be calculated. This will be done by predicting `Detect_Concentraion` between a grid of values - specifically the minimum and maximum of the two predictiors (casing depth and pH). A non-linear line will be drawn on the plot to separate the two classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5ec81-82c6-422f-9f19-4e0f6787649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_grid_df <- function(classification_model, data, resolution = 100) {\n",
    "    # This function predicts the outcome (Detect_Concentration) at evenly spaced data points using the two variables (pH and casing depth)\n",
    "    # to create a decision boundary between the outcome classes (detect and non-detect samples).\n",
    "\n",
    "    # :parameters: a classification-based supervised machine learning model, dataset containing the predictors and outcome variable,\n",
    "    # specifies the number of data points to make between the minimum and maximum predictor values\n",
    "    # :output: a grid of values for both predictors and their corresponding predicted outcome class\n",
    "\n",
    "    # Grabbing only the predictor data\n",
    "    predictor_data <- data[,1:2]\n",
    "    \n",
    "    # Creating a dataframe that contains the min and max for both features\n",
    "    min_max_df <- sapply(predictor_data, range, na.rm = TRUE)\n",
    "\n",
    "    # Creating a vector of evenly spaced points between the min and max for the first variable (casing depth)\n",
    "    variable1_vector <- seq(min_max_df[1,1], min_max_df[2,1], length.out = resolution)\n",
    "    # Creating a vector of evenly spaced points between the min and max for the second variable (pH)\n",
    "    variable2_vector <- seq(min_max_df[1,2], min_max_df[2,2], length.out = resolution)\n",
    "\n",
    "    # Creating a dataframe of grid values by combining the two vectors\n",
    "    grid_df <- data.frame(cbind(rep(variable1_vector, each = resolution), rep(variable2_vector, time = resolution)))\n",
    "    colnames(grid_df) <- colnames(min_max_df)\n",
    "    \n",
    "    # Predicting class label based on all the predictor pairs of data\n",
    "    grid_df$Pred_Class = predict(classification_model, grid_df, type = \"class\")\n",
    "    \n",
    "    return(grid_df)\n",
    "}\n",
    "\n",
    "# calling function\n",
    "grid_df_rf = get_grid_df(rf_detect_arsenic, highest_predictivity_data)\n",
    "head(grid_df_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292804ff-4691-4d70-b756-f11f4591f684",
   "metadata": {},
   "source": [
    "## Decision Boundary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08cd7a1-59d1-4d18-be3f-9ed20e5ee4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=10, repr.plot.height=7) #changing plot size\n",
    "\n",
    "ggplot() +\n",
    "  geom_point(data = arsenic_data, aes(x = pH, y = Casing_Depth, color = Detect_Concentration),\n",
    "            position = position_jitter(w = 0.1, h = 0.1), size = 4, alpha = 0.8) + \n",
    "  geom_contour(data = grid_df_rf, aes(x = pH, y = Casing_Depth, z = as.numeric(Pred_Class == \"D\")), \n",
    "               color = \"black\", breaks = 0.5) + # adds contour line\n",
    "  geom_point(data = grid_df_rf, aes(x = pH, y = Casing_Depth, color = Pred_Class), \n",
    "             size = 0.1) + # shades plot\n",
    "\n",
    "  theme_light() + \n",
    "  theme(axis.line = element_line(color = \"black\"), #making x and y axes black\n",
    "        axis.text = element_text(size = 12), #changing size of x axis labels\n",
    "        axis.title = element_text(face = \"bold\", size = rel(1.7)), #changes axis titles\n",
    "        legend.title = element_text(face = 'bold', size = 14), #changes legend title\n",
    "        legend.text = element_text(size = 12), #changes legend text\n",
    "        legend.position = c(0.15,0.85), # move legend to top left corner\n",
    "        legend.background = element_rect(color = 'black', fill = 'white', linetype = 'solid'), # changes legend background\n",
    "        strip.text.x = element_text(size = 15, face = \"bold\"), #changes size of facet x axis \n",
    "        strip.text.y = element_text(size = 15, face = \"bold\")) + #changes size of facet y axis \n",
    "  labs(y = 'Casing Depth (ft)') + #changing axis labels \n",
    "\n",
    "  scale_color_discrete(name = \"Arsenic Detection\", # renaming the legend\n",
    "                      labels = c('Non-Detect','Detect')) # renaming the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8c956-4788-4560-aef6-025d60cddd4d",
   "metadata": {},
   "source": [
    "There is some overlap between detect and non-detect iAs samples; however, it is still evident that detect iAs samples were more likely to have a more basic pH and have a lower (<80 ft) casing depth.\n",
    "\n",
    "## Concluding Remarks\n",
    "In conclusion, this training module provided methodologies to aid the interpretability of supervised ML with variable importance and decision boundary plots. Variable importance helps quantify the impact of each feature's importance on an algorithm's predictivity. The most important or environmentally-relevant predictors can be selected in a decision boundary plot to further understand and visualize the features impact on the model's classification. \n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "+ [Variable Importance](https://compgenomr.github.io/book/trees-and-forests-random-forests-in-action.html#variable-importance-1)\n",
    "+ [Decision Boundary](https://www.aiforanyone.org/glossary/decision-boundary#:~:text=a%20decision%20boundary%3F-,A%20decision%20boundary%20is%20a%20line%20or%20surface%20that%20separates,make%20predictions%20about%20new%20data.)\n",
    "\n",
    "## Test Your Knowledge\n",
    "1. Using the \"Managanese_Data\", extract the variable importance for each predictor on a RF model. What two features have the highest variable importance?  \n",
    "2. Using casing depth and the feature with the highest variable importance, construct a decision boundary plot. Was RF able to separate Manganese samples based on these two variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c3120-a86d-4427-b0ad-3a5cf4067200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
